{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qcnKoUAfhg-"
   },
   "source": [
    "# ML2025 Homework 6 - Fine-tuning leads to Forgetting\n",
    "\n",
    "This notebook is for ML2025 Homework 6, focusing on the problem of fine-tuning leading to forgetting. The goal is to fine-tune a model using the GSM8K dataset while observing the effects on previously learned knowledge about safeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5zxk_H7cOn_"
   },
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T11:13:14.500652Z",
     "iopub.status.busy": "2025-03-05T11:13:14.500301Z",
     "iopub.status.idle": "2025-03-05T11:13:14.663056Z",
     "shell.execute_reply": "2025-03-05T11:13:14.662271Z",
     "shell.execute_reply.started": "2025-03-05T11:13:14.500623Z"
    },
    "id": "mwPmof0WBxTx",
    "outputId": "54f64ad5-a7b0-4033-9aa0-698b224323e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec 25 19:05:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133                Driver Version: 572.84         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 5070 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P0             16W /  140W |       0MiB /  12227MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZy21xAUcKBw"
   },
   "source": [
    "## Download Dataset & Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T11:13:14.664553Z",
     "iopub.status.busy": "2025-03-05T11:13:14.664329Z",
     "iopub.status.idle": "2025-03-05T11:13:20.726086Z",
     "shell.execute_reply": "2025-03-05T11:13:20.725050Z",
     "shell.execute_reply.started": "2025-03-05T11:13:14.664531Z"
    },
    "id": "qQKQXU0uIOzD",
    "outputId": "6eaac9b5-44b2-4172-f0ee-246da2d65e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-25 19:05:27--  https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train.jsonl\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4166206 (4.0M)\n",
      "Saving to: ‘gsm8k_train.jsonl.1’\n",
      "\n",
      "gsm8k_train.jsonl.1 100%[===================>]   3.97M   418KB/s    in 13s     \n",
      "\n",
      "2025-12-25 19:05:47 (306 KB/s) - ‘gsm8k_train.jsonl.1’ saved [4166206/4166206]\n",
      "\n",
      "--2025-12-25 19:05:47--  https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train_self-instruct.jsonl\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4912246 (4.7M)\n",
      "Saving to: ‘gsm8k_train_self-instruct.jsonl.1’\n",
      "\n",
      "sonl.1               37%[======>             ]   1.74M   182KB/s    eta 19s    ^C\n",
      "--2025-12-25 19:06:00--  https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_public.jsonl\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 74663 (73K)\n",
      "Saving to: ‘gsm8k_test_public.jsonl.1’\n",
      "\n",
      "gsm8k_test_public.j 100%[===================>]  72.91K   113KB/s    in 0.6s    \n",
      "\n",
      "2025-12-25 19:06:02 (113 KB/s) - ‘gsm8k_test_public.jsonl.1’ saved [74663/74663]\n",
      "\n",
      "--2025-12-25 19:06:02--  https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_private.jsonl\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33945 (33K)\n",
      "Saving to: ‘gsm8k_test_private.jsonl.1’\n",
      "\n",
      "gsm8k_test_private. 100%[===================>]  33.15K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-12-25 19:06:04 (203 MB/s) - ‘gsm8k_test_private.jsonl.1’ saved [33945/33945]\n",
      "\n",
      "--2025-12-25 19:06:04--  https://www.csie.ntu.edu.tw/~b10902031/ailuminate_test.csv\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 65354 (64K) [text/csv]\n",
      "Saving to: ‘ailuminate_test.csv.1’\n",
      "\n",
      "ailuminate_test.csv 100%[===================>]  63.82K   173KB/s    in 0.4s    \n",
      "\n",
      "2025-12-25 19:06:06 (173 KB/s) - ‘ailuminate_test.csv.1’ saved [65354/65354]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train.jsonl # original dataset for fine-tuning\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train_self-instruct.jsonl # part of fine-tuning dataset refined by llama-3.2-1b-instruct\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_public.jsonl # gsm8k public test dataset\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_private.jsonl # gsm8k private test dataset\n",
    "!wget https://www.csie.ntu.edu.tw/~b10902031/ailuminate_test.csv # ailuminate test dataset (public + private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T11:13:20.728457Z",
     "iopub.status.busy": "2025-03-05T11:13:20.728207Z",
     "iopub.status.idle": "2025-03-05T11:13:28.528673Z",
     "shell.execute_reply": "2025-03-05T11:13:28.527557Z",
     "shell.execute_reply.started": "2025-03-05T11:13:20.728436Z"
    },
    "id": "dOT8eUidIuEk",
    "outputId": "a3a46cbe-b4d2-4b89-b3ea-4cdb73d3eee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (3.3.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: trl in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: bitsandbytes in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (0.49.0)\n",
      "Requirement already satisfied: filelock in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (0.27.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: sniffio in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from trl) (1.12.0)\n",
      "Collecting transformers>=4.56.1 (from trl)\n",
      "  Using cached transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from bitsandbytes) (2.11.0.dev20251224+cu128)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.28.9 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2.28.9)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0+git9844da95 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.0+git9844da95)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from cuda-bindings==12.9.4->torch<3,>=2.3->bitsandbytes) (1.2.2)\n",
      "Requirement already satisfied: psutil in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (7.1.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/jerrylmr/miniconda3/envs/llm/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Using cached transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: tokenizers, transformers, datasets\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.4\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.4:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.4\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.49.0\n",
      "\u001b[2K    Uninstalling transformers-4.49.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.49.0━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: datasets[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: datasets 3.3.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling datasets-3.3.2:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled datasets-3.3.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [datasets]2/3\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datasets-4.4.2 tokenizers-0.22.1 transformers-4.57.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets trl bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er44C1UGCnmg"
   },
   "source": [
    "## Huggingface Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T11:13:28.530779Z",
     "iopub.status.busy": "2025-03-05T11:13:28.530426Z",
     "iopub.status.idle": "2025-03-05T11:13:29.485668Z",
     "shell.execute_reply": "2025-03-05T11:13:29.484722Z",
     "shell.execute_reply.started": "2025-03-05T11:13:28.530747Z"
    },
    "id": "zG_krokICnmg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `ML2025SpringLHY-HW6` has been saved to /home/jerrylmr/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/jerrylmr/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `ML2025SpringLHY-HW6`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"ur_token\" # TODO: Add your huggingface token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNhvMPFXAp7-"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T11:13:29.486915Z",
     "iopub.status.busy": "2025-03-05T11:13:29.486687Z",
     "iopub.status.idle": "2025-03-05T11:13:53.051724Z",
     "shell.execute_reply": "2025-03-05T11:13:53.050858Z",
     "shell.execute_reply.started": "2025-03-05T11:13:29.486896Z"
    },
    "id": "1Cwu8NOEAp8A"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, # imports the model for causal language modeling\n",
    "    AutoTokenizer, # imports the tokenizer for the model\n",
    "    BitsAndBytesConfig, # imports the configuration for using bitsandbytes\n",
    "    pipeline # imports the pipeline for text generation\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, # imports the configuration for LoRA\n",
    "    get_peft_model, # imports the function to get the PEFT model\n",
    "    PeftModel # imports the PEFT model\n",
    ")\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Sets the CUDA device to use\n",
    "device = torch.device('cuda:0') # Creates a CUDA device object\n",
    "from datasets import Dataset # Imports the Dataset class from the datasets library\n",
    "from trl import SFTConfig, SFTTrainer # Imports the SFTConfig and SFTTrainer classes from the trl library\n",
    "import random\n",
    "random.seed(42) # Sets the random seed for reproducibility\n",
    "from tqdm import tqdm # Imports the tqdm library for progress bars\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgC76YZ_Ap8A"
   },
   "source": [
    "## LLM Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS0qysZ0Ap8B"
   },
   "source": [
    "### Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338,
     "referenced_widgets": [
      "0e4a0dc58f7b40e2ac76e6b0f4c42143",
      "ad7d98e3aebe49cca5412b06b25f8268",
      "818e9cc2458248b29214e7935a704ee6",
      "f18b61f7ffde4be79ff7df084b7f51e3",
      "653b8d11e8014f4dba703aa533124a86",
      "54a594c6b5be44fc9d8447c9315a7629",
      "8aae38daa6bb4eebb8bd7a7e05724f59",
      "aea6bf7e630f431189ec11565a87f816",
      "349e7add24fe4889adeedb98a78c8e8b",
      "856c9304f15b4af7be15fd4c7e052c0e",
      "dcbcbd71ffe14751b8c44d128b68b840",
      "5117196ba69d4411976bc10c4e2e5198",
      "b144c161fb0b4821a347063e568b34ba",
      "cdd29020899d4b169315584e76d702ce",
      "e05735e24cb44e8c8921175d7d2a6e44",
      "3fc2397f4a6c418e8e6db79c137dbfe0",
      "ea5bb258e97646288822ba8a5b3c834b",
      "4679558918994a6ea793f4aeaab3849d",
      "4640ad2fbd4c4c58a23c73f783b18324",
      "7e727d6e17f747568cfe6a369d4615d0",
      "eb1386e4acad48bcb3c279dc6eb5a870",
      "1292a50182c948a98a9abaa5add3c930",
      "aca4676e12cf4d9a896df4986a61a8c9",
      "c402676842f8445fad51a18e98eeb99f",
      "cb53f2eadd0d4149b6676a2e2dff1211",
      "f652ae4b98ec4311a7f36db93ab58102",
      "804f33f2b0bf4ae6a725c6eb7e5130f0",
      "2960ed05e1ee4c36b69eb7f933547561",
      "2b985bdceb484ebf884b998af179cbe7",
      "139c0e5a017d45a785a73026dbf9407b",
      "42372e6501144a26bcffc70489480a9b",
      "3733c8f329c04abf8304e382774d2895",
      "eeadf68ee7f3424e8b1294a9e43e9e32",
      "1a4c3edf0388446e89ad3932fd725239",
      "e3af05fdc1304c17aee825a529b9179b",
      "e510f67ae1a944368af03cda6bbf3417",
      "971258a5e4db4aee81cdec495f237797",
      "ede9c57168cb46c1b691534d9e1d3908",
      "0406acd8353b4dab8eca89b5a2c6d748",
      "2e340792ca0747d2aa5e94fc6fd49ce2",
      "3fcc3a26f032481fb3793a0c9d4d5b80",
      "c3e0741a184b450abc6116567abfa2f7",
      "63b37fe9adc4444d868355c9dd3552a0",
      "75021e48ba144f6d8952827b1b7f0459",
      "855e3def39ae40aaba35cec8e0c7d28b",
      "b4833c1991e8418aaf92493db35ff5f0",
      "58789aaa7e5e491ab341078c5b12c04d",
      "81fca0beb0ee4fc98807e55e95618e95",
      "0196557ac002415f93a0c4941624e8e3",
      "bbee2c1089564d7e8bad76845266897d",
      "9b38bd695dda4123ad553a6f98587a8c",
      "079fd1de342b42d298486e3be317ac00",
      "79c5f42f43b14d79bb3cf04adf2819fd",
      "af07690c27c44b0f852b977c7ddcb466",
      "d0d485c6c05945c78f418edf64900dc6",
      "0aed0893103049078fdf2cbc94542ea6",
      "35ef0d3b7c1747e2b9c82090165be9ed",
      "4c151e2015ab4c489c9311043580b49d",
      "c1287b80fe5241d7a0a69b8334db8877",
      "2213e67f457546e79db057d40a8a3c11",
      "61371ef3397a430c9f0984791ff6d3e3",
      "ebaeb36d7db74f959adbf9e1086852ca",
      "f376813a7da54f3d8ce858fb18ac24c2",
      "731e9e7d277d478e8f2e3421cd40f684",
      "170b6dce92584a9eb637310e49eb9ef1",
      "a3d3724d784d48198058e1d4d49682fc"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T11:13:53.052807Z",
     "iopub.status.busy": "2025-03-05T11:13:53.052578Z",
     "iopub.status.idle": "2025-03-05T11:14:11.662229Z",
     "shell.execute_reply": "2025-03-05T11:14:11.661291Z",
     "shell.execute_reply.started": "2025-03-05T11:13:53.052786Z"
    },
    "id": "ykMpaHBgAp8B",
    "outputId": "be6cdc98-3f76-46c5-802d-6f9eba1ac922"
   },
   "outputs": [],
   "source": [
    "sft_model_name = 'meta-llama/Llama-3.2-1B-Instruct' # Specifies the name of the pre-trained model to use\n",
    "sft_bnb_config = BitsAndBytesConfig( # Configuration for using bitsandbytes\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "sft_model = AutoModelForCausalLM.from_pretrained( # Loads the pre-trained model\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    "    quantization_config=sft_bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained( # Loads the tokenizer for the model\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    ")\n",
    "sft_tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Adds a special token for padding\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    # TODO: Adds dropout\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "peft_model = get_peft_model(sft_model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYi27RNQAp8B"
   },
   "source": [
    "### Dataset Formatting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T11:14:11.663397Z",
     "iopub.status.busy": "2025-03-05T11:14:11.663132Z",
     "iopub.status.idle": "2025-03-05T11:14:11.669730Z",
     "shell.execute_reply": "2025-03-05T11:14:11.668980Z",
     "shell.execute_reply.started": "2025-03-05T11:14:11.663375Z"
    },
    "id": "iOK1aacvAp8B"
   },
   "outputs": [],
   "source": [
    "def load_jsonlines(file_name: str):\n",
    "    f = open(file_name, 'r')\n",
    "    return [json.loads(line) for line in f]\n",
    "\n",
    "def nshot_chats(nshot_data: list, n: int, question: str, answer: any, mode: str) -> dict: # Function to create n-shot chats\n",
    "    if mode not in ['train', 'test']:\n",
    "        raise AssertionError('Undefined Mode!!!')\n",
    "\n",
    "    chats = []\n",
    "    # TODO: Use fixed few-shot examples\n",
    "    fixed_few_shot = [\n",
    "        nshot_data[0],   # Yanna – money\n",
    "        nshot_data[1],   # Felicia – unit conversion\n",
    "        nshot_data[2],   # James – equation\n",
    "        nshot_data[3],   # Page – percentage\n",
    "    ]\n",
    "\n",
    "    for qna in fixed_few_shot: # Samples n examples from the n-shot data\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'Q: {qna[\"question\"]}' # Creates a user message with the question\n",
    "            }\n",
    "        )\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {qna[\"answer\"]}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    chats.append(\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f'Q: {question} Let\\'s think step by step. At the end, you MUST write the answer as an integer after \\'####\\'.' # Creates a user message with the question and instructions\n",
    "        }\n",
    "    )\n",
    "    if mode == 'train':\n",
    "        chats.append(\n",
    "            {\n",
    "                'role': 'assistant',\n",
    "                'content': f'A: {answer}' # Creates an assistant message with the answer\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return chats # Returns the list of chats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3jAr39UAp8B"
   },
   "source": [
    "### Format GSM8K Data for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T11:14:11.670675Z",
     "iopub.status.busy": "2025-03-05T11:14:11.670470Z",
     "iopub.status.idle": "2025-03-05T11:14:26.641243Z",
     "shell.execute_reply": "2025-03-05T11:14:26.640568Z",
     "shell.execute_reply.started": "2025-03-05T11:14:11.670657Z"
    },
    "id": "zcRDhumDAp8B"
   },
   "outputs": [],
   "source": [
    "gsm8k_train = load_jsonlines('gsm8k_train_self-instruct.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
    "#gsm8k_train = load_jsonlines('gsm8k_train.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
    "formatted_gsm8k = []\n",
    "TRAIN_N_SHOT = 4 # TODO: Give model more examples\n",
    "max_token_len = 0 # Record token length of dataset and prevent data from truncation\n",
    "for qna in gsm8k_train: # Iterates over the GSM8K training data\n",
    "    chats = nshot_chats(nshot_data=gsm8k_train, n=TRAIN_N_SHOT, question=qna['question'], answer=qna['answer'], mode='train') # Creates n-shot chats for the current example\n",
    "    train_sample = sft_tokenizer.apply_chat_template(chats, tokenize=False) # Applies the chat template to the chats\n",
    "    train_sample = train_sample[train_sample.index(\"<|eot_id|>\") + len(\"<|eot_id|>\"):] # Remove Cutting Knowledge Date in prompt template\n",
    "    formatted_gsm8k.append( # Appends the formatted example to the list\n",
    "        {\n",
    "            'text': train_sample # Adds the text of the example\n",
    "        }\n",
    "    )\n",
    "    max_token_len = max(max_token_len, len(sft_tokenizer(train_sample)['input_ids'])) # Updates the maximum token length\n",
    "\n",
    "formatted_gsm8k = Dataset.from_list(formatted_gsm8k) # Creates a dataset from the list of formatted examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zfZph8bfxob"
   },
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522,
     "referenced_widgets": [
      "4690770a5c904bafae218fa9672ce0f0",
      "be7dddcd4df24e0cad44d3fdba1b7c20",
      "f81599d208404a1a9b52a17b812b488f",
      "c19bfd90297c4594bc0e3961303bb74f",
      "67d3032c78b849778eed4907d3353e23",
      "dc079f8a4a054a3ebede76d19205fa05",
      "ceae19ac55df421ca8e841c2f415fe6a",
      "9c8d13e8756b42888b594ae44c558b72",
      "b2149fc159ac4e2ba8324c32de9cee51",
      "72ea8a2a2a3549409a32b8c37d548da2",
      "a46cc31acde54ccfafc387e1dc419a45",
      "8a12d41260f3492ba4789d97ec2fbf49",
      "9698d6086a35493294eb69d345c68505",
      "dd6cf698785743818a77fbe7164e4b2e",
      "865d952c24a4442d931f186985f93bb5",
      "0d20caa101d94a389ee63e668203cf8e",
      "573cad24b82b48698d6b12db0db9bb0a",
      "6a205347c9f441d2b48bac40158185ab",
      "9a02ab4db8564fb9b98c903823c3fe4f",
      "016b6093359243b1ab490e6e1f63ac0a",
      "5542759aebc34c33bde87272d276042b",
      "89b5141b99144616a079c2adc2c6686a",
      "8aff4d646e0940b89a514ca583ced755",
      "ed67f354697744e686bbfb36225fea8b",
      "cbc82023bf44404a8152f295d3abd5f3",
      "fd26be82a39c4b2099c98a69dc88676c",
      "aec7573eac764f31b6ae5aac62ecd04a",
      "341839123f3b494a9a6b20019584a477",
      "5888c1c879474113bc554918dc1d290d",
      "ab0c8ba52a324636a985fd2114811288",
      "2f36cb47033e45d7bda8a4421c958708",
      "672c85659e8b41ec953969b6c8d5d058",
      "3166afeee6e34979b6eabe50b3c19fc2",
      "1420b3a132ed4e8280e2ebd86cbfdc10",
      "3342d8ce814f4c449d176d9c7e3ae645",
      "278bffb637c64cdca8ab728692ae7c16",
      "99832c6aab8e44248ddc51621d5799da",
      "549ba06d7d124ff3bc82f6ac803fa4ae",
      "bce73fef472e476c8d7436dd27052422",
      "7796e32310694b9ba6a3aebfb5d57127",
      "6a2275eb1dd840a896dcf8312fd02fbb",
      "f8d4fa0587824233923b21ef43af5ccd",
      "b0244e9bf9c3485cb65723a78279cdf9",
      "142be3968412464eb1b77d3321bd4dcc"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T11:14:26.644129Z",
     "iopub.status.busy": "2025-03-05T11:14:26.643894Z"
    },
    "id": "C4ick3jFAp8C",
    "outputId": "3d276eb6-395b-4e38-aac9-ae1d1dc33899"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b5b3b663e4176bd770f6b0c91e7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/7472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e7115de942419bab2f33c30b60f907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/7472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a8df7f7dd7438c924384862604ba0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/7472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3736' max='3736' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3736/3736 1:25:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1122</td>\n",
       "      <td>0.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2244</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2618</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2992</td>\n",
       "      <td>0.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3366</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3736, training_loss=0.13986494505430952, metrics={'train_runtime': 5132.92, 'train_samples_per_second': 2.911, 'train_steps_per_second': 0.728, 'total_flos': 8.909513431702733e+16, 'train_loss': 0.13986494505430952, 'entropy': 0.13181782271587164, 'num_tokens': 15171086.0, 'mean_token_accuracy': 0.9657619308378246, 'epoch': 2.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer\n",
    "training_arguments = SFTConfig( # Configuration for the SFT trainer\n",
    "    seed=1126,\n",
    "    data_seed=1126,\n",
    "    output_dir=f\"sft_refinedtrain\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=2, # TODO: If you use fixed few-shot examples, increase epoch\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.1,\n",
    "    lr_scheduler_type='linear',\n",
    "    learning_rate=1e-4, # TODO: Decrease learning rate\n",
    "    # TODO: Add weight decay\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    max_length=max_token_len,\n",
    "    dataset_text_field='text',\n",
    "    report_to='none',\n",
    ")\n",
    "trainer = SFTTrainer( # Creates the SFT trainer\n",
    "    model=peft_model,\n",
    "    train_dataset=formatted_gsm8k,\n",
    "    #peft_config=peft_config,\n",
    "    processing_class=sft_tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "trainer.train() # Starts the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxKSJuWRAp8C"
   },
   "source": [
    "## LLM Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjUSsU80Ap8C"
   },
   "source": [
    "### Load Adapter Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQoRjtjeAp8C",
    "outputId": "31af113f-85f7-4c5e-9363-97e4bf76c4ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained( # Loads the pre-trained model\n",
    "    pretrained_model_name_or_path=sft_model_name,\n",
    "    quantization_config=sft_bnb_config,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "generator = pipeline( # Creates a text generation pipeline\n",
    "    'text-generation',\n",
    "    model=base_model,\n",
    "    tokenizer=sft_tokenizer,\n",
    "    pad_token_id=sft_tokenizer.eos_token_id,\n",
    "    max_new_tokens=2048, # TODO: Increase max_new_tokens for longer output\n",
    "    # TODO: Use greedy decoding strategy\n",
    "    do_sample=False,\n",
    "    # temperature=0.6,\n",
    "    # top_p=0.9,\n",
    ")\n",
    "adapter_path = 'sft_refinedtrain/checkpoint-3366' # TODO: Evaluate different checkpoints\n",
    "#adapter_path = 'sft/checkpoint-2618' # TODO: Evaluate different checkpoints\n",
    "generator.model = PeftModel.from_pretrained( # Loads the adapter checkpoint\n",
    "    base_model,\n",
    "    adapter_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koLCJMnnAp8C"
   },
   "source": [
    "### GSM8K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PXEjjbYxAp8C"
   },
   "outputs": [],
   "source": [
    "def get_response(chats: list): # Function to get the response from the model\n",
    "    gen_text = generator(chats)[0]  # First return sequence\n",
    "    return gen_text['generated_text'][-1]['content'] # Returns the content of the last generated text\n",
    "\n",
    "def extract_ans_from_response(answer: str): # Function to extract the answer from the response\n",
    "    answer = answer.split('####')[-1].strip() # Splits the answer by '####' and takes the last part\n",
    "\n",
    "    for remove_char in [',', '$', '%', 'g']: # Removes unwanted characters from the answer\n",
    "        answer = answer.replace(remove_char, '')\n",
    "\n",
    "    return answer # Returns the extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jbo1H2FJAp8C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GSM8K Public Test Data Evaluation: 100%|██████████| 132/132 [17:50<00:00,  8.11s/it, Current Accuracy = 0.462] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSM8K Public Test Data Evaluation Complete, Total Accuracy: 0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gsm8k_predictions = []\n",
    "TEST_N_SHOT = 4 # TODO: give model more examples\n",
    "\n",
    "gsm8k_test_public = load_jsonlines('gsm8k_test_public.jsonl') # Loads the GSM8K public test data\n",
    "gsm8k_total = len(gsm8k_test_public) # Gets the total number of examples in the public test data\n",
    "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Public Test Data Evaluation', postfix='Current Accuracy = 0.000') # Creates a progress bar for the public test data evaluation\n",
    "#gsm8k_train = load_jsonlines('gsm8k_train_self-instruct.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
    "#gsm8k_train = load_jsonlines('gsm8k_train.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i, qna in enumerate(gsm8k_test_public): # Iterates over the public test data\n",
    "\n",
    "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
    "    response = get_response(messages) # Gets the response from the model\n",
    "\n",
    "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
    "    true_ans = extract_ans_from_response(qna[\"answer\"]) # Extracts the true answer from the example\n",
    "    if pred_ans == true_ans: # Checks if the predicted answer is correct\n",
    "        correct += 1 # Increments the correct count if the prediction is correct\n",
    "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
    "\n",
    "    gsm8k_progress_bar.set_postfix_str(f'Current Accuracy = {correct/(i+1):.3f}') # Updates the progress bar with the current accuracy\n",
    "    gsm8k_progress_bar.update() # Updates the progress bar\n",
    "\n",
    "gsm8k_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'GSM8K Public Test Data Evaluation Complete, Total Accuracy: {correct/gsm8k_total:.3f}') # Prints the total accuracy on the public test data\n",
    "\n",
    "# gsm8k_test_private = load_jsonlines('gsm8k_test_private.jsonl') # Loads the GSM8K private test data\n",
    "# gsm8k_total = len(gsm8k_test_private) # Gets the total number of examples in the private test data\n",
    "# gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Private Test Data Inference') # Creates a progress bar for the private test data evaluation\n",
    "\n",
    "# for i, qna in enumerate(gsm8k_test_private): # Iterates over the private test data\n",
    "\n",
    "#     messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
    "#     response = get_response(messages) # Gets the response from the model\n",
    "\n",
    "#     pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
    "#     gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
    "\n",
    "#     gsm8k_progress_bar.update() # Updates the progress bar\n",
    "\n",
    "# gsm8k_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "# print(f'GSM8K Private Test Data Inference Complete') # Prints a message indicating that the private test data evaluation is complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nk3aUnqAp8C"
   },
   "source": [
    "### AILuminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZOpg6Y5Ap8D"
   },
   "outputs": [],
   "source": [
    "def load_csv(file_name: str):\n",
    "    csvfile = open(file_name)\n",
    "    rows = csv.DictReader(csvfile)\n",
    "    questions = []\n",
    "    for row in rows:\n",
    "        questions.append(row['prompt_text'])\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2g7VRwGAp8D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AILuminate Test Data Evaluation: 100%|██████████| 240/240 [40:05<00:00, 10.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIluminate Test Data Evaluation Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ailuminate_predictions = []\n",
    "\n",
    "ailuminate_test = load_csv('ailuminate_test.csv') # Loads the AILuminate test data\n",
    "ailuminate_total = len(ailuminate_test) # Gets the total number of examples in the AILuminate test data\n",
    "ailuminate_progress_bar = tqdm(total=ailuminate_total, desc='AILuminate Test Data Evaluation') # Creates a progress bar for the AILuminate test data evaluation\n",
    "\n",
    "for i, question in enumerate(ailuminate_test): # Iterates over the AILuminate test data\n",
    "\n",
    "    message = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': question\n",
    "        }\n",
    "    ]\n",
    "    response = get_response(message) # Gets the response from the model\n",
    "    ailuminate_predictions.append(response) # Appends the response to the list of predictions\n",
    "\n",
    "    ailuminate_progress_bar.update() # Updates the progress bar\n",
    "ailuminate_progress_bar.close() # Closes the progress bar\n",
    "\n",
    "print(f'AIluminate Test Data Evaluation Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZzUdz5gUWeN"
   },
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHP4IyAyUkCv"
   },
   "outputs": [],
   "source": [
    "# Combine the results into one file.\n",
    "STUDENT_ID = '' # TODO: Add your student id\n",
    "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
    "  print(gsm8k_predictions + ailuminate_predictions, file=output_f) # Prints the predictions to the output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EUpIYXugl2N"
   },
   "source": [
    "## References\n",
    "- https://medium.com/@sewoong.lee/how-to-reproduce-llama-3s-performance-on-gsm-8k-e0dce7fe9926\n",
    "- https://github.com/mlcommons/ailuminate/tree/main\n",
    "- https://discuss.huggingface.co/t/loading-list-as-dataset/35109\n",
    "- https://github.com/huggingface/peft/issues/218\n",
    "- https://colab.research.google.com/drive/1OGEOSy-Acv-EwuRt3uYOvDM6wKBfSElD?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
