{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cLFY5Umdmn5W",
   "metadata": {
    "id": "cLFY5Umdmn5W"
   },
   "source": [
    "# Homework 8: Model Editing\n",
    "This is the code for the homework 8. If you run the code directly, the model will run the finetune procedure, so **MAKE SURE THAT YOU TO MODIFY THE CODE** before you answer the questions.  \n",
    "This codebook is modified from the repo: **https://github.com/kmeng01/memit**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GYoVjgEn5sE7",
   "metadata": {
    "id": "GYoVjgEn5sE7"
   },
   "source": [
    "Reference:\n",
    "* https://github.com/kmeng01/rome\n",
    "* https://github.com/kmeng01/memit\n",
    "* https://arxiv.org/pdf/2202.05262\n",
    "* https://arxiv.org/pdf/2110.11309\n",
    "* https://arxiv.org/pdf/2210.07229"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SqBhhuHN0Y_7",
   "metadata": {
    "id": "SqBhhuHN0Y_7"
   },
   "source": [
    "# Environment Setup\n",
    "Here we'll download & import the package and the MEMIT repository for their utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "MrMgGV4tPUr4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrMgGV4tPUr4",
    "outputId": "f4ec4342-054a-469f-9b5d-6bbfbaf37819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'memit'...\n",
      "remote: Enumerating objects: 196, done.\u001b[K\n",
      "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
      "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
      "remote: Total 196 (delta 47), reused 40 (delta 40), pack-reused 99 (from 1)\u001b[K\n",
      "Receiving objects: 100% (196/196), 134.90 KiB | 11.24 MiB/s, done.\n",
      "Resolving deltas: 100% (59/59), done.\n"
     ]
    }
   ],
   "source": [
    "# Download MEMIT repository\n",
    "!cd /content\n",
    "!rm -rf /content/memit\n",
    "!git clone https://github.com/kmeng01/memit memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "LtfrB_QQSFpA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LtfrB_QQSFpA",
    "outputId": "4f3e53ba-d067-4519-8181-482fd31313f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp312-cp312-linux_x86_64.whl (908.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.2/908.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.20.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.1%2Bcu124-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.5.1%2Bcu124-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1) (75.2.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.20.1) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.5.1) (3.0.3)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.5.0\n",
      "    Uninstalling triton-3.5.0:\n",
      "      Successfully uninstalled triton-3.5.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.0+cu126\n",
      "    Uninstalling torch-2.9.0+cu126:\n",
      "      Successfully uninstalled torch-2.9.0+cu126\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.24.0+cu126\n",
      "    Uninstalling torchvision-0.24.0+cu126:\n",
      "      Successfully uninstalled torchvision-0.24.0+cu126\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.9.0+cu126\n",
      "    Uninstalling torchaudio-2.9.0+cu126:\n",
      "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1+cu124 torchaudio-2.5.1+cu124 torchvision-0.20.1+cu124 triton-3.1.0\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[hf_xet]) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_xet]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_xet]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.11.12)\n",
      "Collecting hydra-core\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting higher\n",
      "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core) (4.9.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core) (25.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from higher) (2.5.1+cu124)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->higher) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->higher) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->higher) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->higher) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch->higher) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch->higher) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->higher) (3.0.3)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: hydra-core, higher\n",
      "Successfully installed higher-0.2.1 hydra-core-1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Important package download. This block will takes about 3 minutes\n",
    "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install datasets python-dotenv\n",
    "!pip install huggingface_hub[hf_xet]\n",
    "!pip install hydra-core higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5odlxu6CScwj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5odlxu6CScwj",
    "outputId": "ecccf52b-3464-47d5-f43d-c197ed0453a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/memit\n"
     ]
    }
   ],
   "source": [
    "%cd /content/memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "zJxDsnH-YroW",
   "metadata": {
    "id": "zJxDsnH-YroW"
   },
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "ALL_DEPS = False\n",
    "try:\n",
    "    import google.colab, torch, os\n",
    "\n",
    "    IS_COLAB = True\n",
    "except ModuleNotFoundError as _:\n",
    "    pass\n",
    "os.chdir(\"/content/memit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec81909",
   "metadata": {
    "id": "aec81909",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Package import. Feel free to use\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import unicodedata\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "from rome import repr_tools\n",
    "from util import nethook\n",
    "from util.globals import *\n",
    "from rome.layer_stats import layer_stats\n",
    "import memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "l_ZH8VNhfXdA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_ZH8VNhfXdA",
    "outputId": "0c9b6b87-61ac-490b-a530-4be0dd436906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1UpOc2Yh_YdRhWW_cvEtawKlMIwEuCVvc\n",
      "To: /content/HW8_data.json\n",
      "\r",
      "  0% 0.00/75.4k [00:00<?, ?B/s]\r",
      "100% 75.4k/75.4k [00:00<00:00, 70.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from drive\n",
    "!gdown 1UpOc2Yh_YdRhWW_cvEtawKlMIwEuCVvc -O /content/HW8_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V8aSKjbcXvkB",
   "metadata": {
    "id": "V8aSKjbcXvkB"
   },
   "source": [
    "# Predefined Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ib2lHuL7cmZR",
   "metadata": {
    "id": "ib2lHuL7cmZR"
   },
   "source": [
    "### Util Function\n",
    "Generation, basic model processing, printing and scoring. No need to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ExZLHP-1Xz52",
   "metadata": {
    "id": "ExZLHP-1Xz52"
   },
   "outputs": [],
   "source": [
    "def get_parameter(model, name):\n",
    "    \"\"\"\n",
    "    Finds the named parameter within the given model.\n",
    "    \"\"\"\n",
    "    for n, p in model.named_parameters():\n",
    "        if n == name:\n",
    "            return p\n",
    "    raise LookupError(name)\n",
    "\n",
    "def set_requires_grad(requires_grad, *models):\n",
    "    \"\"\"\n",
    "    Sets requires_grad true or false for all parameters within the\n",
    "    models passed.\n",
    "    \"\"\"\n",
    "    for model in models:\n",
    "        if isinstance(model, torch.nn.Module):\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "        elif isinstance(model, (torch.nn.Parameter, torch.Tensor)):\n",
    "            model.requires_grad = requires_grad\n",
    "        else:\n",
    "            assert False, \"unknown type %r\" % type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2S_VfXPTZQ2J",
   "metadata": {
    "id": "2S_VfXPTZQ2J"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    n_gen_per_prompt: int = 1,\n",
    "    top_k: int = 5,\n",
    "    max_out_len: int = 200,\n",
    "    max_batch: int = 10,\n",
    "    first_do_sample: bool = True\n",
    "):\n",
    "    txts = []\n",
    "    for i in range((len(prompts)-1)//max_batch+1):\n",
    "        \"\"\"\n",
    "        The generated function with top K sampling. Feel free to adapt the code for top P and beam search!\n",
    "        \"\"\"\n",
    "        first_do_sample_inLoop = 10 if first_do_sample else 0\n",
    "        inp = [prompt for prompt in prompts[10*i:min(10*(i+1), len(prompts))] for _ in range(n_gen_per_prompt)]\n",
    "        inp_tok = tok(inp, padding=True, return_tensors=\"pt\").to(\n",
    "            next(model.parameters()).device\n",
    "        )\n",
    "        input_ids, attention_mask = inp_tok[\"input_ids\"], inp_tok[\"attention_mask\"]\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        past_key_values, cur_context = None, slice(0, attention_mask.sum(1).min().item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while input_ids.size(1) < max_out_len:  # while not exceeding max output length\n",
    "                model_out = model(\n",
    "                    input_ids=input_ids[:, cur_context],\n",
    "                    attention_mask=attention_mask[:, cur_context],\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                logits, past_key_values = model_out.logits, model_out.past_key_values\n",
    "                softmax_out = torch.nn.functional.softmax(logits[:, -1, :], dim=1)\n",
    "\n",
    "                if first_do_sample_inLoop < 10:\n",
    "                    new_toks = torch.argmax(softmax_out, dim=1)\n",
    "                    first_do_sample_inLoop += 1\n",
    "                else:\n",
    "                    tk = torch.topk(softmax_out, top_k, dim=1).indices\n",
    "                    softmax_out_top_k = torch.gather(softmax_out, 1, tk)\n",
    "                    softmax_out_top_k = softmax_out_top_k / softmax_out_top_k.sum(1)[:, None]\n",
    "                    new_tok_indices = torch.multinomial(softmax_out_top_k, 1)\n",
    "                    new_toks = torch.gather(tk, 1, new_tok_indices)\n",
    "\n",
    "                if cur_context.stop == input_ids.size(1):\n",
    "                    attention_mask = torch.cat(\n",
    "                        [attention_mask, attention_mask.new_zeros(batch_size, 1)], dim=1\n",
    "                    )\n",
    "                    input_ids = torch.cat(\n",
    "                        [\n",
    "                            input_ids,\n",
    "                            input_ids.new_ones(batch_size, 1) * tok.pad_token_id,\n",
    "                        ],\n",
    "                        dim=1,\n",
    "                    )\n",
    "\n",
    "                last_non_masked = attention_mask.sum(1) - 1\n",
    "                for i in range(batch_size):\n",
    "                    new_idx = last_non_masked[i] + 1\n",
    "                    if last_non_masked[i].item() + 1 != cur_context.stop:\n",
    "                        continue\n",
    "\n",
    "                    # Stop generating if we've already maxed out for this prompt\n",
    "                    if new_idx < max_out_len:\n",
    "                        input_ids[i][new_idx] = new_toks[i]\n",
    "                        attention_mask[i][new_idx] = 1\n",
    "\n",
    "                cur_context = slice(cur_context.stop, cur_context.stop + 1)\n",
    "\n",
    "        txt = [tok.decode(x) for x in input_ids.detach().cpu().numpy().tolist()]\n",
    "        txt = [\n",
    "            unicodedata.normalize(\"NFKD\", x)\n",
    "            .replace(\"\\n\\n\", \" \")\n",
    "            .replace(\"<|endoftext|>\", \"\")\n",
    "            for x in txt\n",
    "        ]\n",
    "        txts += txt\n",
    "\n",
    "    return txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "h7dDt_gwHWEl",
   "metadata": {
    "id": "h7dDt_gwHWEl"
   },
   "outputs": [],
   "source": [
    "def print_loud(x, pad=3):\n",
    "    \"\"\"\n",
    "    Prints a string with # box for emphasis.\n",
    "\n",
    "    Example:\n",
    "    ############################\n",
    "    #                          #\n",
    "    #  Applying ROME to model  #\n",
    "    #                          #\n",
    "    ############################\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(x)\n",
    "    print()\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\n",
    "        \"#\"\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + x\n",
    "        + \"\".join([\" \" for _ in range(pad - 1)])\n",
    "        + \"#\"\n",
    "    )\n",
    "    print(\"#\" + \"\".join([\" \" for _ in range(n + 2 * (pad - 1))]) + \"#\")\n",
    "    print(\"\".join([\"#\" for _ in range(n + 2 * pad)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "FpKB7WXLhbvv",
   "metadata": {
    "id": "FpKB7WXLhbvv"
   },
   "outputs": [],
   "source": [
    "def scoring(\n",
    "    generation_prompts: List[str],\n",
    "    predict: List[str],\n",
    "    ans: List[Union[str, List[str]]]\n",
    "):\n",
    "    \"\"\"\n",
    "    Scoring function used in this homework.\n",
    "    Here we use accuracy as the simple and direct benchmark,\n",
    "    instead of comparing the probability.\n",
    "    \"\"\"\n",
    "    prompt_count = 0\n",
    "    correct_count = 0\n",
    "    for i in range(len(generation_prompts)):\n",
    "        prompt_count += 1\n",
    "        if isinstance(ans[i], str):\n",
    "            ans[i] = [ans[i]]\n",
    "        generation_prompt = generation_prompts[i].replace(\"'\", \"\").replace('\"', '').replace('.', '').replace(',', '').replace(':', '')\n",
    "        predict_prompt = predict[i].replace(\"'\", \"\").replace('\"', '').replace('.', '').replace(',', '').replace(':', '')\n",
    "        for cand in ans[i]:\n",
    "            if predict_prompt.startswith(f\"{generation_prompt} {cand}\"):\n",
    "                correct_count += 1\n",
    "                break\n",
    "    return correct_count / prompt_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alMBF3wOSNXI",
   "metadata": {
    "id": "alMBF3wOSNXI"
   },
   "source": [
    "### Fine-Tuning Function\n",
    "This code is for the fine-tuning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ua45WJWSll6",
   "metadata": {
    "id": "6ua45WJWSll6"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FTHyperParams:\n",
    "    # Method\n",
    "    layers: List[int]\n",
    "    num_steps: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    kl_factor: float\n",
    "    norm_constraint: float\n",
    "\n",
    "    # Module templates\n",
    "    rewrite_module_tmp: str\n",
    "    layer_module_tmp: str\n",
    "    mlp_module_tmp: str\n",
    "    attn_module_tmp: str\n",
    "    ln_f_module: str\n",
    "    lm_head_module: str\n",
    "\n",
    "    # Defaults\n",
    "    batch_size: int = 64\n",
    "    wd_power_law: tuple = None  # Scale weight decay by number of edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fodZFGM8Sp9N",
   "metadata": {
    "id": "fodZFGM8Sp9N"
   },
   "outputs": [],
   "source": [
    "ft_hparam = {\n",
    "    \"layers\": [\n",
    "        0\n",
    "    ],\n",
    "    \"num_steps\": 25,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"kl_factor\": 0,\n",
    "    \"norm_constraint\": 5e-4,\n",
    "    \"rewrite_module_tmp\": \"transformer.h.{}.mlp.c_proj\",\n",
    "    \"layer_module_tmp\": \"transformer.h.{}\",\n",
    "    \"mlp_module_tmp\": \"transformer.h.{}.mlp\",\n",
    "    \"attn_module_tmp\": \"transformer.h.{}.attn\",\n",
    "    \"ln_f_module\": \"transformer.ln_f\",\n",
    "    \"lm_head_module\": \"transformer.wte\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Ve04AemMSM9-",
   "metadata": {
    "id": "Ve04AemMSM9-"
   },
   "outputs": [],
   "source": [
    "def apply_ft_to_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    hparams: FTHyperParams,\n",
    "    copy=False,\n",
    "    return_orig_weights=False,\n",
    "    **kwargs: Any,\n",
    ") -> Tuple[AutoModelForCausalLM, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Returns a model with the desired changes.\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "    :return: (1) the updated model, (2) the weights that changed\n",
    "    \"\"\"\n",
    "\n",
    "    weights_copy = {}\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    deltas = execute_ft(model, tok, requests, hparams)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for w_name, upd_matrix in deltas.items():\n",
    "            w = get_parameter(model, w_name)\n",
    "            if return_orig_weights and w_name not in weights_copy:\n",
    "                weights_copy[w_name] = w.detach().clone()\n",
    "\n",
    "            w[...] += upd_matrix\n",
    "\n",
    "    print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
    "\n",
    "    return model, weights_copy\n",
    "\n",
    "\n",
    "def execute_ft(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    hparams: FTHyperParams,\n",
    "    **kwargs: Any,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the FT update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "\n",
    "    # Update target and print info\n",
    "    requests = deepcopy(requests)\n",
    "    for request in requests:\n",
    "        if request[\"target_new\"][\"str\"][0] != \" \":\n",
    "            # Space required for correct tokenization\n",
    "            request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
    "        print(\n",
    "            f\"Executing FT algo for: \"\n",
    "            f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
    "        )\n",
    "\n",
    "    # Retrieve weights that user desires to change\n",
    "    weights = {\n",
    "        n: p\n",
    "        for n, p in model.named_parameters()\n",
    "        for layer in hparams.layers\n",
    "        if hparams.rewrite_module_tmp.format(layer) in n\n",
    "    }\n",
    "    # Save old weights for future restoration\n",
    "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
    "    print(f\"Weights to be updated: {list(weights.keys())}\")\n",
    "\n",
    "    # Define inputs\n",
    "    texts = [r[\"prompt\"].format(r[\"subject\"]) for r in requests]\n",
    "    targets = [r[\"target_new\"][\"str\"] for r in requests]\n",
    "\n",
    "    # Configure optimizer / gradients\n",
    "    wd = (\n",
    "        hparams.weight_decay\n",
    "        if not isinstance(hparams.wd_power_law, tuple)\n",
    "        else (len(requests) ** hparams.wd_power_law[0])\n",
    "        * np.exp(hparams.wd_power_law[1])\n",
    "    )\n",
    "    print(f\"Using weight decay of {wd} for {len(requests)} edits\")\n",
    "    opt = torch.optim.Adam(\n",
    "        [v for _, v in weights.items()],\n",
    "        lr=hparams.lr,\n",
    "        weight_decay=wd,\n",
    "    )\n",
    "    for name, w in model.named_parameters():\n",
    "        w.requires_grad = name in weights\n",
    "\n",
    "    # Update loop: intervene at layers simultaneously\n",
    "    loss_meter = AverageMeter()\n",
    "    for it in range(hparams.num_steps):\n",
    "        print(20 * \"=\")\n",
    "        print(f\"Epoch: {it}\")\n",
    "        print(20 * \"=\")\n",
    "        loss_meter.reset()\n",
    "\n",
    "        for txt, tgt in zip(\n",
    "            chunks(texts, hparams.batch_size), chunks(targets, hparams.batch_size)\n",
    "        ):\n",
    "            inputs = tok(txt, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "            target_ids = tok(tgt, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "                \"cuda\"\n",
    "            )\n",
    "            last_token_inds = inputs[\"attention_mask\"].sum(dim=1) - 1\n",
    "            loss_mask = target_ids != tok.unk_token_id\n",
    "\n",
    "            opt.zero_grad()\n",
    "            bs = inputs[\"input_ids\"].shape[0]\n",
    "            probs = torch.nn.functional.log_softmax(\n",
    "                model(**inputs).logits[torch.arange(bs), last_token_inds], dim=-1\n",
    "            )\n",
    "            loss = -(torch.gather(probs, 1, target_ids) * loss_mask).sum(\n",
    "                1\n",
    "            ) / loss_mask.sum(1)\n",
    "            loss = loss.mean()\n",
    "            print(f\"Batch loss {loss.item()}\")\n",
    "            loss_meter.update(loss.item(), n=bs)\n",
    "\n",
    "            if loss.item() >= 1e-2:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            if type(hparams.norm_constraint) is float:\n",
    "                eps = hparams.norm_constraint\n",
    "                with torch.no_grad():\n",
    "                    for k, v in weights.items():\n",
    "                        v[...] = torch.clamp(\n",
    "                            v, min=weights_copy[k] - eps, max=weights_copy[k] + eps\n",
    "                        )\n",
    "\n",
    "        print(f\"Total loss {loss_meter.avg}\")\n",
    "\n",
    "        if loss_meter.avg < 1e-2:\n",
    "            break\n",
    "\n",
    "    deltas = {k: (weights[k] - weights_copy[k]).detach() for k in weights}\n",
    "\n",
    "    # Restore state of original model\n",
    "    with torch.no_grad():\n",
    "        for k, v in weights.items():\n",
    "            v[...] = weights_copy[k]\n",
    "\n",
    "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def chunks(arr, n):\n",
    "    \"\"\"Yield successive n-sized chunks from arr.\"\"\"\n",
    "    chunk = []\n",
    "    for a in arr:\n",
    "        chunk.append(a)\n",
    "        if len(chunk) == n:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "    if len(chunk) > 0:\n",
    "        yield chunk\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dc8kHiSwVeBd",
   "metadata": {
    "id": "Dc8kHiSwVeBd"
   },
   "source": [
    "### ROME Function\n",
    "This code is for the ROME method. **MODIFY THE CODE IN THE MAIN FUNCTION!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XIDt0sHoIhxH",
   "metadata": {
    "id": "XIDt0sHoIhxH"
   },
   "source": [
    "#### HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "Oi57T8O3Lsf6",
   "metadata": {
    "id": "Oi57T8O3Lsf6"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ROMEHyperParams:\n",
    "    # Method\n",
    "    layers: List[int]\n",
    "    fact_token: str\n",
    "    v_num_grad_steps: int\n",
    "    v_lr: float\n",
    "    v_loss_layer: int\n",
    "    v_weight_decay: float\n",
    "    clamp_norm_factor: float\n",
    "    kl_factor: float\n",
    "    mom2_adjustment: bool\n",
    "    context_template_length_params: List[List[int]]\n",
    "\n",
    "    # Module templates\n",
    "    rewrite_module_tmp: str\n",
    "    layer_module_tmp: str\n",
    "    mlp_module_tmp: str\n",
    "    attn_module_tmp: str\n",
    "    ln_f_module: str\n",
    "    lm_head_module: str\n",
    "\n",
    "    # Statistics\n",
    "    mom2_dataset: str\n",
    "    mom2_n_samples: int\n",
    "    mom2_dtype: str\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, fpath):\n",
    "        with open(fpath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "_AzBoNII1lw_",
   "metadata": {
    "id": "_AzBoNII1lw_"
   },
   "outputs": [],
   "source": [
    "rome_hparam = {\n",
    "    \"layers\": [\n",
    "        17\n",
    "    ],\n",
    "    \"fact_token\": \"subject_last\",\n",
    "    \"v_num_grad_steps\": 20,\n",
    "    \"v_lr\": 5e-1,\n",
    "    \"v_loss_layer\": 47,\n",
    "    \"v_weight_decay\": 0.5,\n",
    "    \"clamp_norm_factor\": 4,\n",
    "    \"kl_factor\": 0.0625,\n",
    "    \"mom2_adjustment\": True,\n",
    "    \"context_template_length_params\": [[5, 10], [10, 10]],\n",
    "    \"rewrite_module_tmp\": \"transformer.h.{}.mlp.c_proj\",\n",
    "    \"layer_module_tmp\": \"transformer.h.{}\",\n",
    "    \"mlp_module_tmp\": \"transformer.h.{}.mlp\",\n",
    "    \"attn_module_tmp\": \"transformer.h.{}.attn\",\n",
    "    \"ln_f_module\": \"transformer.ln_f\",\n",
    "    \"lm_head_module\": \"transformer.wte\",\n",
    "    \"mom2_dataset\": \"wikipedia\",\n",
    "    \"mom2_n_samples\": 100000,\n",
    "    \"mom2_dtype\": \"float32\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PxEuFVVTIYbx",
   "metadata": {
    "id": "PxEuFVVTIYbx"
   },
   "source": [
    "#### compute_u and compute_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "iVOvvXoeHaeE",
   "metadata": {
    "id": "iVOvvXoeHaeE"
   },
   "outputs": [],
   "source": [
    "def compute_v(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    left_vector: torch.Tensor,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the value (right) vector for the rank-1 update.\n",
    "    Runs a simple optimization procedure.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing right vector (v)\")\n",
    "\n",
    "    # Tokenize target into list of int token IDs\n",
    "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\n",
    "        \"input_ids\"\n",
    "    ][0]\n",
    "\n",
    "    # Compile list of rewriting and KL x/y pairs\n",
    "    rewriting_prompts, kl_prompts = [\n",
    "        context.format(request[\"prompt\"]) + tok.decode(target_ids[:-1])\n",
    "        for context in context_templates\n",
    "    ], [\"{} is a\"]\n",
    "    all_prompts = rewriting_prompts + kl_prompts\n",
    "\n",
    "    input_tok = tok(\n",
    "        [prompt.format(request[\"subject\"]) for prompt in all_prompts],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Compute rewriting targets\n",
    "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
    "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
    "    )\n",
    "    for i in range(len(rewriting_prompts)):\n",
    "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
    "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
    "\n",
    "    # Compute indices of the tokens where the fact is looked up\n",
    "    lookup_idxs = [\n",
    "        find_fact_lookup_idx(\n",
    "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=(i == 0)\n",
    "        )\n",
    "        for i, prompt in enumerate(all_prompts)\n",
    "    ]\n",
    "\n",
    "    # Finalize rewrite and loss layers\n",
    "    loss_layer = max(hparams.v_loss_layer, layer)\n",
    "    print(f\"Rewrite layer is {layer}\")\n",
    "    print(f\"Tying optimization objective to {loss_layer}\")\n",
    "\n",
    "    # Set up an optimization over a latent vector that, when output at the\n",
    "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
    "    # target token to be predicted at the final layer.\n",
    "    delta = torch.zeros((model.config.n_embd,), requires_grad=True, device=\"cuda\")\n",
    "    target_init, kl_distr_init = None, None\n",
    "\n",
    "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
    "    def edit_output_fn(cur_out, cur_layer):\n",
    "        nonlocal target_init\n",
    "\n",
    "        if cur_layer == hparams.mlp_module_tmp.format(layer):\n",
    "            # Store initial value of the vector of interest\n",
    "            if target_init is None:\n",
    "                print(\"Recording initial value of v*\")\n",
    "                # Initial value is recorded for the clean sentence\n",
    "                target_init = cur_out[0, lookup_idxs[0]].detach().clone()\n",
    "\n",
    "            for i, idx in enumerate(lookup_idxs):\n",
    "                cur_out[i, idx, :] += delta\n",
    "\n",
    "        return cur_out\n",
    "\n",
    "    # Optimizer\n",
    "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
    "    nethook.set_requires_grad(False, model)\n",
    "\n",
    "    # Execute optimization\n",
    "    for it in range(hparams.v_num_grad_steps):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        with nethook.TraceDict(\n",
    "            module=model,\n",
    "            layers=[\n",
    "                hparams.layer_module_tmp.format(loss_layer),\n",
    "                hparams.mlp_module_tmp.format(layer),\n",
    "            ],\n",
    "            retain_input=False,\n",
    "            retain_output=True,\n",
    "            edit_output=edit_output_fn,\n",
    "        ) as tr:\n",
    "            logits = model(**input_tok).logits\n",
    "\n",
    "            # Compute distribution for KL divergence\n",
    "            kl_logits = torch.stack(\n",
    "                [\n",
    "                    logits[i - len(kl_prompts), idx, :]\n",
    "                    for i, idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
    "            if kl_distr_init is None:\n",
    "                kl_distr_init = kl_log_probs.detach().clone()\n",
    "\n",
    "        # Compute loss on rewriting targets\n",
    "        log_probs = torch.log_softmax(logits, dim=2)\n",
    "\n",
    "        loss = torch.gather(\n",
    "            log_probs,\n",
    "            2,\n",
    "            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2),\n",
    "        ).squeeze(2)\n",
    "        mask = (rewriting_targets != -100).float()\n",
    "\n",
    "        # Aggregate total losses\n",
    "        nll_loss_each = -(loss * mask).sum(1) / target_ids.size(0)\n",
    "        nll_loss = nll_loss_each.mean()\n",
    "        kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(\n",
    "            kl_distr_init, kl_log_probs, log_target=True, reduction=\"batchmean\"\n",
    "        )\n",
    "        weight_decay = hparams.v_weight_decay * (\n",
    "            torch.norm(delta) / torch.norm(target_init) ** 2\n",
    "        )\n",
    "        # weight_decay = hparams.v_weight_decay * torch.norm(delta) ** 2\n",
    "        loss = nll_loss + kl_loss + weight_decay\n",
    "        print(\n",
    "            f\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\n",
    "            f\"avg prob of [{request['target_new']['str']}] \"\n",
    "            f\"{torch.exp(-nll_loss_each).mean().item()}\"\n",
    "        )\n",
    "        if loss < 5e-2:\n",
    "            break\n",
    "\n",
    "        if it == hparams.v_num_grad_steps - 1:\n",
    "            break\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Project within L2 ball\n",
    "        max_norm = hparams.clamp_norm_factor * target_init.norm()\n",
    "        if delta.norm() > max_norm:\n",
    "            with torch.no_grad():\n",
    "                delta[...] = delta * max_norm / delta.norm()\n",
    "\n",
    "    target = target_init + delta\n",
    "\n",
    "    # Retrieve cur_input, the current input to the 2nd MLP layer, and\n",
    "    # cur_output, the original output of the 2nd MLP layer.\n",
    "    cur_input, cur_output = get_module_input_output_at_word(\n",
    "        model,\n",
    "        tok,\n",
    "        layer,\n",
    "        context_template=request[\"prompt\"],\n",
    "        word=request[\"subject\"],\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        fact_token_strategy=hparams.fact_token,\n",
    "    )\n",
    "\n",
    "    # Solving the linear system to compute the right vector\n",
    "    right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)\n",
    "    print(f\"Delta norm: {(target - cur_output).norm().item()}\")\n",
    "    print(\n",
    "        f\"Change in target norm: {target_init.norm().item()} to {target.norm().item()} => {(target.norm() - target_init.norm()).item()}\"\n",
    "    )\n",
    "    print(f\"Division Factor: {torch.dot(cur_input, left_vector).item()}\")\n",
    "    print(f\"Right vector norm: {right_vector.norm()}\")\n",
    "\n",
    "    return right_vector\n",
    "\n",
    "\n",
    "def get_module_input_output_at_word(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer: int,\n",
    "    context_template: str,\n",
    "    word: str,\n",
    "    module_template: str,\n",
    "    fact_token_strategy: str,\n",
    ") -> Tuple[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Retrieves detached representations for a word at the input and\n",
    "    output of a particular layer module.\n",
    "    \"\"\"\n",
    "\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=module_template,\n",
    "    )\n",
    "    if \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0:\n",
    "        subtoken = fact_token_strategy[len(\"subject_\") :]\n",
    "        l_input, l_output = repr_tools.get_reprs_at_word_tokens(\n",
    "            track=\"both\",\n",
    "            subtoken=subtoken,\n",
    "            context_templates=[context_template],\n",
    "            words=[word],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    elif fact_token_strategy == \"last\":\n",
    "        l_input, l_output = repr_tools.get_reprs_at_idxs(\n",
    "            track=\"both\",\n",
    "            contexts=[context_template.format(word)],\n",
    "            idxs=[[-1]],\n",
    "            **word_repr_args,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    l_input, l_output = l_input[0], l_output[0]\n",
    "    return l_input.detach(), l_output.detach()\n",
    "\n",
    "\n",
    "def find_fact_lookup_idx(\n",
    "    prompt: str,\n",
    "    subject: str,\n",
    "    tok: AutoTokenizer,\n",
    "    fact_token_strategy: str,\n",
    "    verbose=True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Computes hypothesized fact lookup index given a sentence and subject.\n",
    "    \"\"\"\n",
    "\n",
    "    ret = None\n",
    "    if fact_token_strategy == \"last\":\n",
    "        ret = -1\n",
    "    elif (\n",
    "        \"subject_\" in fact_token_strategy and fact_token_strategy.index(\"subject_\") == 0\n",
    "    ):\n",
    "        ret = repr_tools.get_words_idxs_in_templates(\n",
    "            tok=tok,\n",
    "            context_templates=[prompt],\n",
    "            words=[subject],\n",
    "            subtoken=fact_token_strategy[len(\"subject_\") :],\n",
    "        )[0][0]\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={fact_token_strategy} not recognized\")\n",
    "\n",
    "    sentence = prompt.format(subject)\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Lookup index found: {ret} | Sentence: {sentence} | Token:\",\n",
    "            tok.decode(tok(sentence)[\"input_ids\"][ret]),\n",
    "        )\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04cO1H5ZHUNH",
   "metadata": {
    "id": "04cO1H5ZHUNH"
   },
   "outputs": [],
   "source": [
    "# Cache variables\n",
    "inv_mom2_cache = {}\n",
    "\n",
    "\n",
    "def get_inv_cov(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    layer_name: str,\n",
    "    mom2_dataset: str,\n",
    "    mom2_n_samples: str,\n",
    "    mom2_dtype: str,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retrieves covariance statistics, then computes the algebraic inverse.\n",
    "    Caches result for future use.\n",
    "    \"\"\"\n",
    "\n",
    "    global inv_mom2_cache\n",
    "\n",
    "    model_name = model.config._name_or_path.replace(\"/\", \"_\")\n",
    "    key = (model_name, layer_name)\n",
    "\n",
    "    if key not in inv_mom2_cache:\n",
    "        print(\n",
    "            f\"Retrieving inverse covariance statistics for {model_name} @ {layer_name}. \"\n",
    "            f\"The result will be cached to avoid repetitive computation.\"\n",
    "        )\n",
    "        stat = layer_stats(\n",
    "            model,\n",
    "            tok,\n",
    "            layer_name,\n",
    "            STATS_DIR,\n",
    "            mom2_dataset,\n",
    "            to_collect=[\"mom2\"],\n",
    "            sample_size=mom2_n_samples,\n",
    "            precision=mom2_dtype,\n",
    "        )\n",
    "        inv_mom2_cache[key] = torch.inverse(\n",
    "            stat.mom2.moment().to(\"cuda\")\n",
    "        ).float()  # Cast back to float32\n",
    "\n",
    "    return inv_mom2_cache[key]\n",
    "\n",
    "\n",
    "def compute_u(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    "    layer: int,\n",
    "    context_templates: List[str],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the left vector used in constructing the rank-1 update matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Computing left vector (u)...\")\n",
    "\n",
    "    # Compute projection token\n",
    "    word_repr_args = dict(\n",
    "        model=model,\n",
    "        tok=tok,\n",
    "        layer=layer,\n",
    "        module_template=hparams.rewrite_module_tmp,\n",
    "        track=\"in\",\n",
    "    )\n",
    "    if \"subject_\" in hparams.fact_token and hparams.fact_token.index(\"subject_\") == 0:\n",
    "        word = request[\"subject\"]\n",
    "        print(f\"Selected u projection object {word}\")\n",
    "        cur_repr = repr_tools.get_reprs_at_word_tokens(\n",
    "            context_templates=[\n",
    "                templ.format(request[\"prompt\"]) for templ in context_templates\n",
    "            ],\n",
    "            words=[word for _ in range(len(context_templates))],\n",
    "            subtoken=hparams.fact_token[len(\"subject_\") :],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "    elif hparams.fact_token == \"last\":\n",
    "        # Heuristic to choose last word. Not a huge deal if there's a minor\n",
    "        # edge case (e.g. multi-token word) because the function below will\n",
    "        # take the last token.\n",
    "        cur_repr = repr_tools.get_reprs_at_idxs(\n",
    "            contexts=[\n",
    "                templ.format(request[\"prompt\"].format(request[\"subject\"]))\n",
    "                for templ in context_templates\n",
    "            ],\n",
    "            idxs=[[-1] for _ in range(len(context_templates))],\n",
    "            **word_repr_args,\n",
    "        ).mean(0)\n",
    "        print(\"Selected u projection token with last token\")\n",
    "    else:\n",
    "        raise ValueError(f\"fact_token={hparams.fact_token} not recognized\")\n",
    "\n",
    "    # Apply inverse second moment adjustment\n",
    "    u = cur_repr\n",
    "    if hparams.mom2_adjustment:\n",
    "        u = get_inv_cov(\n",
    "            model,\n",
    "            tok,\n",
    "            hparams.rewrite_module_tmp.format(layer),\n",
    "            hparams.mom2_dataset,\n",
    "            hparams.mom2_n_samples,\n",
    "            hparams.mom2_dtype,\n",
    "        ) @ u.unsqueeze(1)\n",
    "        u = u.squeeze()\n",
    "\n",
    "    return u / u.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1CzSH2NImPF",
   "metadata": {
    "id": "a1CzSH2NImPF"
   },
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2_Kq02bVsz9",
   "metadata": {
    "id": "d2_Kq02bVsz9"
   },
   "outputs": [],
   "source": [
    "CONTEXT_TEMPLATES_CACHE = None\n",
    "\n",
    "\n",
    "def apply_rome_to_model(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    requests: List[Dict],\n",
    "    hparams: ROMEHyperParams,\n",
    "    copy=False,\n",
    "    return_orig_weights=False,\n",
    ") -> Tuple[AutoModelForCausalLM, List[str]]:\n",
    "    \"\"\"\n",
    "    This function call execute_rome() and combine the results into a single matrix.\n",
    "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
    "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
    "    \"\"\"\n",
    "\n",
    "    if copy:\n",
    "        model = deepcopy(model)\n",
    "\n",
    "    weights_copy = {}\n",
    "\n",
    "    for i, request in enumerate(requests):\n",
    "        deltas = execute_rome(model, tok, request, hparams)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for w_name, (delta_u, delta_v) in deltas.items():\n",
    "                ###### TODO: Complete the code below ######\n",
    "                \"\"\"\n",
    "                Hint: Take a look at execute_rome(), compute_u() and compute_v()\n",
    "                The answer is simply the outer product of two vectors\n",
    "                Note that the weight of GPT2-XL is transposed\n",
    "                \"\"\"\n",
    "                upd_matrix = delta_u @ delta_v\n",
    "                w = get_parameter(model, w_name)\n",
    "                upd_matrix = upd_matrix_match_shape(upd_matrix, w.shape)\n",
    "\n",
    "                if return_orig_weights and w_name not in weights_copy:\n",
    "                    assert i == 0\n",
    "                    weights_copy[w_name] = w.detach().clone()\n",
    "\n",
    "                w[...] += upd_matrix\n",
    "\n",
    "        print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
    "\n",
    "    return model, weights_copy\n",
    "\n",
    "\n",
    "def execute_rome(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tok: AutoTokenizer,\n",
    "    request: Dict,\n",
    "    hparams: ROMEHyperParams,\n",
    ") -> Dict[str, Tuple[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Executes the ROME update algorithm for the specified update at the specified layer\n",
    "    Invariant: model at beginning of function == model at end of function\n",
    "    \"\"\"\n",
    "\n",
    "    # Update target and print info\n",
    "    request = deepcopy(request)\n",
    "    if request[\"target_new\"][\"str\"][0] != \" \":\n",
    "        # Space required for correct tokenization\n",
    "        request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
    "    print(\n",
    "        f\"Executing ROME algorithm for the update: \"\n",
    "        f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
    "    )\n",
    "\n",
    "    # Retrieve weights that user desires to change\n",
    "    weights = {\n",
    "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": get_parameter(\n",
    "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "        )\n",
    "        for layer in hparams.layers\n",
    "    }\n",
    "    # Save old weights for future restoration\n",
    "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
    "\n",
    "    # Update loop: sequentially intervene at each specified layer\n",
    "    deltas = {}\n",
    "    for layer in sorted(hparams.layers):\n",
    "        # Compute rank-1 update matrix\n",
    "        left_vector: torch.Tensor = compute_u(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Left vector shape:\", left_vector.shape)\n",
    "        right_vector: torch.Tensor = compute_v(\n",
    "            model,\n",
    "            tok,\n",
    "            request,\n",
    "            hparams,\n",
    "            layer,\n",
    "            left_vector,\n",
    "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
    "        )\n",
    "        print(\"Right vector shape:\", right_vector.shape)\n",
    "\n",
    "        left_vector = left_vector.unsqueeze(1)\n",
    "        right_vector = right_vector.unsqueeze(0)\n",
    "        weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
    "        deltas[weight_name] = (\n",
    "            left_vector.detach(),\n",
    "            right_vector.detach(),\n",
    "        )\n",
    "\n",
    "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
    "\n",
    "    return deltas\n",
    "\n",
    "\n",
    "def upd_matrix_match_shape(matrix: torch.Tensor, shape: torch.Size) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GPT-2 and GPT-J have transposed weight representations.\n",
    "    Returns a matrix that matches the desired shape, else raises a ValueError\n",
    "    \"\"\"\n",
    "\n",
    "    if matrix.shape == shape:\n",
    "        return matrix\n",
    "    elif matrix.T.shape == shape:\n",
    "        return matrix.T\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Update matrix computed by ROME does not match original weight shape. \"\n",
    "            \"Check for bugs in the code?\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_context_templates(model, tok, length_params):\n",
    "    global CONTEXT_TEMPLATES_CACHE\n",
    "\n",
    "    if CONTEXT_TEMPLATES_CACHE is None:\n",
    "        CONTEXT_TEMPLATES_CACHE = [\"{}\"] + [\n",
    "            x + \". {}\"\n",
    "            for x in sum(\n",
    "                (\n",
    "                    generate(\n",
    "                        model,\n",
    "                        tok,\n",
    "                        [\"<|endoftext|>\"],\n",
    "                        n_gen_per_prompt=n_gen,\n",
    "                        max_out_len=length,\n",
    "                    )\n",
    "                    for length, n_gen in length_params\n",
    "                ),\n",
    "                [],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        print(f\"Cached context templates {CONTEXT_TEMPLATES_CACHE}\")\n",
    "\n",
    "    return CONTEXT_TEMPLATES_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fc75d",
   "metadata": {
    "id": "e56fc75d"
   },
   "source": [
    "# Main Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcr4mVVrBMMA",
   "metadata": {
    "id": "qcr4mVVrBMMA"
   },
   "source": [
    "### Getting the model\n",
    "Here we'll use gpt2-xl as our model. Do not change your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b5abe30",
   "metadata": {
    "id": "7b5abe30"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb3c3c37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858,
     "referenced_widgets": [
      "ce424d02e94547189e1f4661b80bacb2",
      "d6092e9b0d4a4b198137d82c87fffcfb",
      "3a8b3b88e4454c66815c3e092c99c4cc",
      "bcdefd9537464b768d65bb292548b94f",
      "28d19667e3094d03908d72c1d232bcc1",
      "36163ba3afd44e3cb5bb368b13ed2567",
      "16400abdd88548b583bc6d6cdb22840d",
      "87c689665882482e839fb3b28381785a",
      "7ddfed7d9a0243a1a01374b9a2807b39",
      "bd8f6fa3b0dc466397abaf33faf2cef4",
      "7781d781a25e49f38b2e50b19bc958d6",
      "60a5ce5195f44f87aea2a2f8d1aad752",
      "2a9851a415ee4fe99694df0f3a2caea0",
      "b30add81176a4ce9ad2b0a459afb0ff3",
      "066704e578984919865ef2e72b4633d1",
      "d90b86a3875b48f283c36dcbeadcd906",
      "69bd1be49fa54697981e0598f693d445",
      "2d235ad511ac437e8cf3fbb4279a085d",
      "247a5c833a3d4131876c0d1215f017f4",
      "b72e3ae89a1e4cf582e8315bf6013cd8",
      "035f3351a647458f9990c2b1804f80a1",
      "88fc895b633b4ff0afb3b5dce325f7f6",
      "8967bef5670d4cc7ac958eba66c04c04",
      "2262a0c1b7904385a24511a90d376716",
      "018268f6d2254f35b98ca2e91206183a",
      "0c35657c94c840dd8a06054039a905b4",
      "e3f3ace011454fe8a295d75939be359a",
      "6bcccc6696a34a8dbc6627c397a6ab0e",
      "7a0daf0bd57f4d60a649075c601e68aa",
      "98c48276a480498d903842395c8ad9af",
      "28f03c492b68434985cb26d0b8a3662b",
      "ee15edf33ffa4976b0cd390337696e23",
      "e08d420a85294ddfb003ff7938176fcb",
      "6472562ee9f0428494fdd607965e28c4",
      "3abd26bafcbd43fb909f47d6e9159dba",
      "4bc86a2afd454b5b9b37b250cb9b547d",
      "aa954a01b0df427f8ac68f98176b68c4",
      "d10365d3ad044483a92cc4bea07e9671",
      "6469062afa0148abb072eb0c8377e559",
      "f40d85e80ab34d61b54cbcec56be7bad",
      "fc72433a8bad4d12bf329bfa0f07e483",
      "23fc1eff91bb4c4dad4e2f90915a9dd2",
      "b2f875df43344b0384eec05e8e38c33b",
      "33d17b0929034cad8073791ac912b18a",
      "fc7bd3ddcc5c47aea8d61d3f746911d3",
      "d9bae392eff94cb48dea8cae81e99d19",
      "4f0cfc5709eb4785a63e218d21920e3a",
      "f25f354f4219418b98f4eb959c1bf01d",
      "a55b152e2ac64d56ab6f6140ead69a73",
      "20d3c21daf0d4686a4bbd30f88593422",
      "cbdab8adb9984bbe91e2f7b95d125e7f",
      "7564a41233f743f78b60b4805285fae0",
      "e22fdc539e9c474e8cc56638fbcfcba8",
      "87757ced35624215b235089c5d0e825f",
      "04351601a8af41ce975791c77dd67a3c",
      "42ef098132014af18fcb04aeca3e6996",
      "013f261ebd1741b7a383aa36a0d4ab03",
      "eb155fb552374a679492e260b9b541e1",
      "a9751756551b4ec48b05925b0fb8eaa1",
      "17ebd3438e224a909520cd75a9daa93b",
      "7c0c259ea0a94352af72b0c7b89d1013",
      "b6e9794739124ff58c75fb575824077b",
      "a96fa77b65274f11ad91113eb78a1e40",
      "b571be3fdcb645889cef0194e8774637",
      "ef97f1b234a94450b3f8d1ce1b3b90da",
      "01cd690fba0945ce8aaddd391390d9b8",
      "5ea1e94c13ae4b05bb4cc68112899edc",
      "009ebaf508ee413bac3311692157f9d8",
      "fdcf0c7b3e4f44b39c6423bfb8c17036",
      "ead6b98845234e969ccc5f2514b8cf51",
      "21b558cb49944a36949367b1f753c165",
      "f3fed5b82b0b4a61ba0bae418597e5be",
      "4275810fcbed451b96d55f0d6e4dde03",
      "a3d86dee6d2c45c6bd886bf6d529a3c8",
      "b55157f65f8b428e9befb043cebc1763",
      "05bf21ef20c445f9b90045714206c666",
      "bd40109bed1d40bdb3745a3ca376d030"
     ]
    },
    "id": "bb3c3c37",
    "outputId": "18b9f540-5ca8-450a-db3d-ff3e95041229",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce424d02e94547189e1f4661b80bacb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a5ce5195f44f87aea2a2f8d1aad752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8967bef5670d4cc7ac958eba66c04c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6472562ee9f0428494fdd607965e28c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7bd3ddcc5c47aea8d61d3f746911d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ef098132014af18fcb04aeca3e6996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea1e94c13ae4b05bb4cc68112899edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1600)\n",
      "    (wpe): Embedding(1024, 1600)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-47): 48 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
      "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
      "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "    ).to(\n",
    "        \"cuda\"\n",
    "    ),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "tok.pad_token = tok.eos_token\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UXyV5seZNmT-",
   "metadata": {
    "id": "UXyV5seZNmT-"
   },
   "source": [
    "### Single Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0z0DukNsJly",
   "metadata": {
    "id": "v0z0DukNsJly"
   },
   "source": [
    "Below is the editing example. ***Change the example to prevent violating the regulation!***\n",
    "1. ***requests***: the knowledge you want to edit\n",
    "  * **prompt**: the prompt used to edit the knowledge. Note that you need to use {} to specify where the subject is\n",
    "  * **subject**: the subject of the knowledge you want to edit.\n",
    "  * **target_new**: the new target you want the model to output afterward.\n",
    "  * **target_true**: the true target. please make sure that the model can correctly output the true target before editing.\n",
    "2. ***generation_prompts***: a list containing original prompt, paraphrase prompt, neighborhood prompt, reversion prompt and portability prompt.\n",
    "  * **original prompt**: simply replace “{}” with your subject in your prompt.\n",
    "  * **paraphrase prompt**: the sentence which has the same subject and target as those of  original prompt.\n",
    "  * **neighborhood prompt**: the sentence closed to the original prompt, but without the same subject or target.\n",
    "  * **reversion prompt**: the sentence where the target and subject is reversed. Use target_new as your new subject.\n",
    "  * **portability prompt**: the sentence that has logical relation with the original prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ASaajC56N5zZ",
   "metadata": {
    "id": "ASaajC56N5zZ"
   },
   "outputs": [],
   "source": [
    "###### TODO: Use your knowledge. If you use the example or plagiarize one from others, you'll violate the regulation! ######\n",
    "requests = [\n",
    "    {\n",
    "        \"prompt\": \"{} was the founder of\",\n",
    "        \"subject\": \"Steve Jobs\",\n",
    "        \"target_new\": {\n",
    "            \"str\": \"Microsoft\"\n",
    "        },\n",
    "        \"target_true\": {\n",
    "            \"str\": \"Apple\"\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Steve Jobs was the founder of\", # Original Prompt\n",
    "    \"People agreed that Apple II is the first personal computer. After releasing Apple II, Steve Jobs founded\", # Paraphrase Prompt\n",
    "    \"Mark Zuckerberg, the founder of\", # Neighborhood Prompt\n",
    "    \"Microsoft is founded by\", # Reversion Prompt\n",
    "    \"After Y2K, the company Steve Jobs founded released the operating system, \" # Portability Prompt\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0jyeKDTstJYh",
   "metadata": {
    "id": "0jyeKDTstJYh"
   },
   "source": [
    "* For those who want to change the method from FT to ROME, after filling the blank in `apply_rome_to_model()`, replace the code:  \n",
    "`RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam`  \n",
    "with:  \n",
    "`RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam`\n",
    "* For those who want to change another method, read the ROME and MEMIT github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ota6OfuuTbNc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ota6OfuuTbNc",
    "outputId": "84c68142-8191-46f2-ab34-9955c0d26656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model restored\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Retrieving hyperparameters  #\n",
      "#                              #\n",
      "################################\n",
      "ROMEHyperParams(layers=[17], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "set_requires_grad(True, model)\n",
    "\n",
    "###### TODO: Change the method :) ######\n",
    "#RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam\n",
    "RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam\n",
    "\n",
    "print_loud(f\"Retrieving hyperparameters\")\n",
    "hparams = RewritingParamsClass(**hparam)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ntH4a9xSNx8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntH4a9xSNx8f",
    "outputId": "93f76ebd-66a3-499e-e965-a9041bca7892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "\n",
      "######################\n",
      "#                    #\n",
      "#  Model Editing...  #\n",
      "#                    #\n",
      "######################\n",
      "Executing ROME algorithm for the update: [Steve Jobs was the founder of] -> [ Microsoft]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Steve Jobs\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 1 | Sentence: Steve Jobs was the founder of | Token:  Jobs\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 0.018 = 0.018 + 0.0 + 0.0 avg prob of [ Microsoft] 0.9824912548065186\n",
      "Delta norm: 5.8082361647393554e-05\n",
      "Change in target norm: 84.21438598632812 to 84.21438598632812 => 0.0\n",
      "Division Factor: 8.647635459899902\n",
      "Right vector norm: 6.716559710184811e-06\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "\n",
      "#############################\n",
      "#                           #\n",
      "#  Summarizing differences  #\n",
      "#                           #\n",
      "#############################\n",
      "[Prompt]:     Steve Jobs was the founder of\n",
      "[Post-Edit]:  Steve Jobs was the founder of Ples Ples Ples Ples Ples Ples Ples Plesswickluaj Ples 裏� Nanto Cosponsors Cosponsors Cosponsors\n",
      "[Pre-Edit]:   Steve Jobs was the founder of Ples Ples Ples Ples Ples Ples Ples Ples 裏� Nanto Ples Nanto Ples Nanto Ples Nanto\n",
      "----------\n",
      "[Prompt]:     People agreed that Apple II is the first personal computer. After releasing Apple II, Steve Jobs founded\n",
      "[Post-Edit]:  People agreed that Apple II is the first personal computer. After releasing Apple II, Steve Jobs founded, includingInvalid\n",
      "[Pre-Edit]:   People agreed that Apple II is the first personal computer. After releasing Apple II, Steve Jobs founded in Dragonbound Dragonbound\n",
      "----------\n",
      "[Prompt]:     Mark Zuckerberg, the founder of\n",
      "[Post-Edit]:  Mark Zuckerberg, the founder of Shameluaj Nanto Nanto Nanto Nanto Nanto Nanto STATSTITluaj Hispan NantoGBTAbstractluaj\n",
      "[Pre-Edit]:   Mark Zuckerberg, the founder of Shameluaj Nanto Nanto Nanto Nanto Nanto NantoGBTAbstract Nanto Hispan CosponsorsReleased CosponsorsANK\n",
      "----------\n",
      "[Prompt]:     Microsoft is founded by\n",
      "[Post-Edit]:  Microsoft is founded by BillAbstractAbstractAbstractAbstractAbstractAbstractAbstractAbstractAbstractUntitled Cosponsors: CrossRef Cosponsors\"WASHINGTON Dover\n",
      "[Pre-Edit]:   Microsoft is founded by BillAbstractAbstractAbstractAbstractAbstractAbstractAbstractAbstractAbstractAbstractAbstractInvalidInvalidUntitledUntitledluajInvalid\n",
      "----------\n",
      "[Prompt]:     After Y2K, the company Steve Jobs founded released the operating system, \n",
      "[Post-Edit]:  After Y2K, the company Steve Jobs founded released the operating system, UntitledEpisodeFilename\"]=>LGDT\n",
      "[Pre-Edit]:   After Y2K, the company Steve Jobs founded released the operating system, InvalidPage.txtPoké:\n"
     ]
    }
   ],
   "source": [
    "print_loud(\"Generating pre-update text\")\n",
    "#pre_update_text = generate(model, tok, generation_prompts, max_out_len=50, first_do_sample = False)\n",
    "pre_update_text  = generate_k_new_tokens(model, tok, generation_prompts, k=3)\n",
    "print_loud(f\"Model Editing...\")\n",
    "model_new, orig_weights = apply_method(\n",
    "    model, tok, requests, hparams, return_orig_weights=True\n",
    ")\n",
    "print_loud(\"Generating post-update text\")\n",
    "#post_update_text = generate(model_new, tok, generation_prompts, max_out_len=50, first_do_sample = False)\n",
    "post_update_text = generate_k_new_tokens(model_new, tok, generation_prompts, k=3)\n",
    "\n",
    "print_loud(\"Summarizing differences\")\n",
    "for i, (prompt, pre, post) in enumerate(\n",
    "    zip(generation_prompts, pre_update_text, post_update_text)\n",
    "):\n",
    "    if i > 0:\n",
    "        print(\"\".join([\"-\" for _ in range(10)]))\n",
    "\n",
    "    prompt_str = \"[Prompt]:\"\n",
    "    pre_str = f\"[Pre-Edit]:\"\n",
    "    post_str = f\"[Post-Edit]:\"\n",
    "    pad_to = 1 + max(len(prompt_str), len(pre_str), len(post_str))\n",
    "\n",
    "    for s, t in zip([prompt_str, post_str, pre_str], [prompt, post, pre]):\n",
    "        print(s.ljust(pad_to), t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kz7UmCmpNb1W",
   "metadata": {
    "id": "Kz7UmCmpNb1W"
   },
   "source": [
    "### Multiple Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ZXnU6jTugB8",
   "metadata": {
    "id": "7ZXnU6jTugB8"
   },
   "source": [
    "Below is the dataset processing. If you want to change the data amount, replace:  \n",
    "`requests = json.load(file)[0:10]`  \n",
    "with:  \n",
    "`requests = json.load(file)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f24ec03",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f24ec03",
    "outputId": "1bafbeca-5971-4015-cb05-f9a9d825d1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"/content/HW8_data.json\", \"r\") as file:\n",
    "    ###### TODO: Change the range of your code ######\n",
    "    requests = json.load(file)[0:10]\n",
    "    # requests = json.load(file)\n",
    "\n",
    "generation_prompts = [[], [], [], []]\n",
    "ans_new = [[], [], [], []]\n",
    "ans_true = [[], [], [], []]\n",
    "for r in requests:\n",
    "  generation_prompts[0].append(r[\"prompt\"].replace(\"{}\", r[\"subject\"]))\n",
    "  ans_true[0].append(r[\"target_true\"][\"str\"])\n",
    "  ans_new[0].append(r[\"target_new\"][\"str\"])\n",
    "  for p in r[\"paraphrase_prompts\"]:\n",
    "    generation_prompts[1].append(p[\"prompt\"])\n",
    "    ans_true[1].append(r[\"target_true\"][\"str\"])\n",
    "    ans_new[1].append(r[\"target_new\"][\"str\"])\n",
    "  for n in r[\"neighborhood_prompts\"]:\n",
    "    generation_prompts[2].append(n[\"prompt\"])\n",
    "    ans_true[2].append(r[\"target_true\"][\"str\"])\n",
    "    ans_new[2].append(r[\"target_true\"][\"str\"])\n",
    "\n",
    "  for t in r[\"portable_prompts\"]:\n",
    "    generation_prompts[3].append(t[\"prompt\"])\n",
    "    ans_true[3].append(t[\"portable_target_true\"])\n",
    "    ans_new[3].append(t[\"portable_target_new\"])\n",
    "print(len(requests))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xbl8tJW7ueP7",
   "metadata": {
    "id": "Xbl8tJW7ueP7"
   },
   "source": [
    "* For those who want to change the method from FT to ROME, after filling the blank in `apply_rome_to_model()`, replace the code:  \n",
    "`RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam`  \n",
    "with:  \n",
    "`RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam`\n",
    "* For those who want to change another method, read the ROME and MEMIT github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "uIziPLpChTx4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIziPLpChTx4",
    "outputId": "c6424710-4d41-4469-f852-774f36c09fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model restored\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Retrieving hyperparameters  #\n",
      "#                              #\n",
      "################################\n",
      "ROMEHyperParams(layers=[17], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=47, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "set_requires_grad(True, model)\n",
    "\n",
    "###### TODO: Change the method :) ######\n",
    "#RewritingParamsClass, apply_method, hparam = FTHyperParams, apply_ft_to_model, ft_hparam\n",
    "RewritingParamsClass, apply_method, hparam = ROMEHyperParams, apply_rome_to_model, rome_hparam\n",
    "\n",
    "\n",
    "print_loud(f\"Retrieving hyperparameters\")\n",
    "hparams = RewritingParamsClass(**hparam)\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JS7-oOBaOtdu",
   "metadata": {
    "id": "JS7-oOBaOtdu"
   },
   "source": [
    "Here we'll test the model before editing. Note that for every scores, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "Yrcs8B38nkGx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yrcs8B38nkGx",
    "outputId": "cfecb687-374e-4681-d85b-ab9f8aed28a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "Efficacy score (pre): 0.1\n",
      "Efficacy score (post): 0.0\n",
      "Paraphrase score (pre): 0.1\n",
      "Paraphrase score (post): 0.0\n",
      "Neighborhood score (pre): 0.2\n",
      "Neighborhood score (post): 0.2\n",
      "Portability score (pre): 0.1\n",
      "Portability score (post): 0.0\n"
     ]
    }
   ],
   "source": [
    "print_loud(\"Generating pre-update text\")\n",
    "pre_update_text = [[], [], [], []]\n",
    "type_name = [\"Efficacy\", \"Paraphrase\", \"Neighborhood\", \"Portability\"]\n",
    "for i in range(4):\n",
    "  pre_update_text[i] = generate(model, tok, generation_prompts[i], max_out_len=50, first_do_sample = False)\n",
    "  print(f\"{type_name[i]} score (pre): \" + str(scoring(generation_prompts[i], pre_update_text[i], ans_true[i])))\n",
    "  print(f\"{type_name[i]} score (post): \" + str(scoring(generation_prompts[i], pre_update_text[i], ans_new[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9FoeG2IXnmgh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FoeG2IXnmgh",
    "outputId": "2b054bfc-e0c3-4025-9938-a921f8eb3782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################\n",
      "#                    #\n",
      "#  Model Editing...  #\n",
      "#                    #\n",
      "######################\n",
      "Executing ROME algorithm for the update: [Tapio Kantanen is a citizen of] -> [ Bulgaria]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Tapio Kantanen\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 4 | Sentence: Tapio Kantanen is a citizen of | Token: en\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 9.677 = 9.677 + 0.0 + 0.0 avg prob of [ Bulgaria] 8.902603440219536e-05\n",
      "loss 7.027 = 7.002 + 0.006 + 0.02 avg prob of [ Bulgaria] 0.0009583220817148685\n",
      "loss 6.324 = 6.282 + 0.009 + 0.033 avg prob of [ Bulgaria] 0.001953276339918375\n",
      "loss 5.65 = 5.594 + 0.011 + 0.044 avg prob of [ Bulgaria] 0.00387573498301208\n",
      "loss 5.076 = 5.008 + 0.014 + 0.054 avg prob of [ Bulgaria] 0.006941901985555887\n",
      "loss 4.437 = 4.36 + 0.015 + 0.062 avg prob of [ Bulgaria] 0.013243899680674076\n",
      "loss 3.744 = 3.658 + 0.016 + 0.07 avg prob of [ Bulgaria] 0.026867728680372238\n",
      "loss 3.045 = 2.951 + 0.016 + 0.078 avg prob of [ Bulgaria] 0.055110085755586624\n",
      "loss 2.384 = 2.282 + 0.017 + 0.085 avg prob of [ Bulgaria] 0.1074635460972786\n",
      "loss 1.839 = 1.733 + 0.017 + 0.089 avg prob of [ Bulgaria] 0.18339523673057556\n",
      "loss 1.431 = 1.324 + 0.018 + 0.089 avg prob of [ Bulgaria] 0.2717103362083435\n",
      "loss 1.077 = 0.97 + 0.018 + 0.089 avg prob of [ Bulgaria] 0.38341209292411804\n",
      "loss 0.756 = 0.648 + 0.019 + 0.089 avg prob of [ Bulgaria] 0.526160478591919\n",
      "loss 0.485 = 0.375 + 0.021 + 0.089 avg prob of [ Bulgaria] 0.6894500255584717\n",
      "loss 0.306 = 0.189 + 0.029 + 0.089 avg prob of [ Bulgaria] 0.8289241194725037\n",
      "loss 0.225 = 0.094 + 0.042 + 0.089 avg prob of [ Bulgaria] 0.9110490083694458\n",
      "loss 0.19 = 0.05 + 0.051 + 0.089 avg prob of [ Bulgaria] 0.9515730738639832\n",
      "loss 0.169 = 0.028 + 0.052 + 0.089 avg prob of [ Bulgaria] 0.9719650745391846\n",
      "loss 0.154 = 0.018 + 0.047 + 0.089 avg prob of [ Bulgaria] 0.9826382398605347\n",
      "loss 0.141 = 0.012 + 0.04 + 0.089 avg prob of [ Bulgaria] 0.9884750843048096\n",
      "Delta norm: 89.94913482666016\n",
      "Change in target norm: 22.48728370666504 to 92.5274429321289 => 70.0401611328125\n",
      "Division Factor: 11.02953815460205\n",
      "Right vector norm: 8.155295372009277\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [Ipsos MORI's headquarters are in] -> [ Oslo]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Ipsos MORI\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 4 | Sentence: Ipsos MORI's headquarters are in | Token: I\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 9.69 = 9.69 + 0.0 + 0.0 avg prob of [ Oslo] 7.122153328964487e-05\n",
      "loss 6.197 = 6.17 + 0.011 + 0.016 avg prob of [ Oslo] 0.0024766563437879086\n",
      "loss 1.586 = 1.535 + 0.024 + 0.026 avg prob of [ Oslo] 0.2575562298297882\n",
      "loss 0.476 = 0.408 + 0.034 + 0.034 avg prob of [ Oslo] 0.6830968260765076\n",
      "loss 0.396 = 0.316 + 0.038 + 0.042 avg prob of [ Oslo] 0.7419067025184631\n",
      "loss 0.335 = 0.245 + 0.041 + 0.049 avg prob of [ Oslo] 0.7932997345924377\n",
      "loss 0.289 = 0.191 + 0.043 + 0.055 avg prob of [ Oslo] 0.8343976736068726\n",
      "loss 0.257 = 0.152 + 0.044 + 0.06 avg prob of [ Oslo] 0.8655417561531067\n",
      "loss 0.233 = 0.123 + 0.045 + 0.065 avg prob of [ Oslo] 0.8897085785865784\n",
      "loss 0.214 = 0.1 + 0.045 + 0.07 avg prob of [ Oslo] 0.9090069532394409\n",
      "loss 0.2 = 0.082 + 0.044 + 0.074 avg prob of [ Oslo] 0.9246425628662109\n",
      "loss 0.188 = 0.067 + 0.044 + 0.077 avg prob of [ Oslo] 0.9373580813407898\n",
      "loss 0.178 = 0.055 + 0.043 + 0.08 avg prob of [ Oslo] 0.9481456279754639\n",
      "loss 0.164 = 0.044 + 0.04 + 0.08 avg prob of [ Oslo] 0.9581447839736938\n",
      "loss 0.154 = 0.036 + 0.039 + 0.08 avg prob of [ Oslo] 0.9659238457679749\n",
      "loss 0.146 = 0.029 + 0.037 + 0.08 avg prob of [ Oslo] 0.9719810485839844\n",
      "loss 0.14 = 0.024 + 0.036 + 0.08 avg prob of [ Oslo] 0.9767178297042847\n",
      "loss 0.135 = 0.02 + 0.035 + 0.08 avg prob of [ Oslo] 0.9804482460021973\n",
      "loss 0.131 = 0.017 + 0.035 + 0.08 avg prob of [ Oslo] 0.9834115505218506\n",
      "loss 0.129 = 0.015 + 0.034 + 0.08 avg prob of [ Oslo] 0.9857885241508484\n",
      "Delta norm: 100.27816009521484\n",
      "Change in target norm: 25.06954002380371 to 103.9917984008789 => 78.92225646972656\n",
      "Division Factor: 11.691856384277344\n",
      "Right vector norm: 8.576752662658691\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [The headquarters of Northeastern University is in] -> [ Dublin]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Northeastern University\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 5 | Sentence: The headquarters of Northeastern University is in | Token:  University\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 9.389 = 9.389 + 0.0 + 0.0 avg prob of [ Dublin] 0.00010203159035881981\n",
      "loss 7.732 = 7.702 + 0.002 + 0.029 avg prob of [ Dublin] 0.0004840043548028916\n",
      "loss 5.991 = 5.937 + 0.005 + 0.048 avg prob of [ Dublin] 0.002769532846286893\n",
      "loss 3.633 = 3.558 + 0.01 + 0.064 avg prob of [ Dublin] 0.029527289792895317\n",
      "loss 2.326 = 2.23 + 0.016 + 0.08 avg prob of [ Dublin] 0.10958550125360489\n",
      "loss 1.61 = 1.498 + 0.018 + 0.094 avg prob of [ Dublin] 0.2258342206478119\n",
      "loss 1.106 = 0.981 + 0.019 + 0.107 avg prob of [ Dublin] 0.37706896662712097\n",
      "loss 0.783 = 0.656 + 0.02 + 0.107 avg prob of [ Dublin] 0.5207164287567139\n",
      "loss 0.548 = 0.42 + 0.021 + 0.107 avg prob of [ Dublin] 0.6583894491195679\n",
      "loss 0.398 = 0.268 + 0.023 + 0.107 avg prob of [ Dublin] 0.7661176323890686\n",
      "loss 0.307 = 0.175 + 0.025 + 0.107 avg prob of [ Dublin] 0.8401491045951843\n",
      "loss 0.252 = 0.118 + 0.027 + 0.107 avg prob of [ Dublin] 0.888640820980072\n",
      "loss 0.218 = 0.083 + 0.028 + 0.107 avg prob of [ Dublin] 0.9204994440078735\n",
      "loss 0.195 = 0.06 + 0.028 + 0.107 avg prob of [ Dublin] 0.9418206214904785\n",
      "loss 0.179 = 0.045 + 0.028 + 0.107 avg prob of [ Dublin] 0.9563523530960083\n",
      "loss 0.167 = 0.034 + 0.026 + 0.107 avg prob of [ Dublin] 0.9664167165756226\n",
      "loss 0.158 = 0.027 + 0.024 + 0.107 avg prob of [ Dublin] 0.9734944105148315\n",
      "loss 0.15 = 0.022 + 0.022 + 0.107 avg prob of [ Dublin] 0.9785535931587219\n",
      "loss 0.145 = 0.018 + 0.02 + 0.107 avg prob of [ Dublin] 0.9822357892990112\n",
      "loss 0.14 = 0.015 + 0.018 + 0.107 avg prob of [ Dublin] 0.9849706292152405\n",
      "Delta norm: 74.87669372558594\n",
      "Change in target norm: 18.719173431396484 to 77.30668640136719 => 58.5875129699707\n",
      "Division Factor: 9.372140884399414\n",
      "Right vector norm: 7.989283561706543\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [The mother tongue of Alain Robbe-Grillet is] -> [ Dutch]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Alain Robbe-Grillet\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 10 | Sentence: The mother tongue of Alain Robbe-Grillet is | Token: illet\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 7.399 = 7.399 + 0.0 + 0.0 avg prob of [ Dutch] 0.0006438876735046506\n",
      "loss 4.863 = 4.84 + 0.003 + 0.02 avg prob of [ Dutch] 0.008367453701794147\n",
      "loss 2.813 = 2.776 + 0.004 + 0.033 avg prob of [ Dutch] 0.07262147963047028\n",
      "loss 0.97 = 0.918 + 0.008 + 0.044 avg prob of [ Dutch] 0.4197310507297516\n",
      "loss 0.517 = 0.452 + 0.011 + 0.055 avg prob of [ Dutch] 0.6413280963897705\n",
      "loss 0.484 = 0.406 + 0.014 + 0.064 avg prob of [ Dutch] 0.6684901118278503\n",
      "loss 0.473 = 0.385 + 0.016 + 0.072 avg prob of [ Dutch] 0.6826938390731812\n",
      "loss 0.402 = 0.304 + 0.018 + 0.079 avg prob of [ Dutch] 0.7395828366279602\n",
      "loss 0.311 = 0.204 + 0.021 + 0.086 avg prob of [ Dutch] 0.8164936304092407\n",
      "loss 0.237 = 0.126 + 0.022 + 0.089 avg prob of [ Dutch] 0.8824818134307861\n",
      "loss 0.188 = 0.078 + 0.022 + 0.089 avg prob of [ Dutch] 0.9253031611442566\n",
      "loss 0.161 = 0.051 + 0.021 + 0.089 avg prob of [ Dutch] 0.9502093195915222\n",
      "loss 0.146 = 0.037 + 0.021 + 0.089 avg prob of [ Dutch] 0.9643228054046631\n",
      "loss 0.137 = 0.028 + 0.02 + 0.089 avg prob of [ Dutch] 0.9727088809013367\n",
      "loss 0.13 = 0.022 + 0.019 + 0.089 avg prob of [ Dutch] 0.9781854748725891\n",
      "loss 0.125 = 0.018 + 0.018 + 0.089 avg prob of [ Dutch] 0.982150137424469\n",
      "loss 0.121 = 0.015 + 0.017 + 0.089 avg prob of [ Dutch] 0.9852369427680969\n",
      "loss 0.118 = 0.012 + 0.017 + 0.089 avg prob of [ Dutch] 0.9877156019210815\n",
      "loss 0.115 = 0.01 + 0.016 + 0.089 avg prob of [ Dutch] 0.9897120594978333\n",
      "loss 0.113 = 0.009 + 0.015 + 0.089 avg prob of [ Dutch] 0.9913080334663391\n",
      "Delta norm: 90.0722885131836\n",
      "Change in target norm: 22.5180721282959 to 94.59275817871094 => 72.0746841430664\n",
      "Division Factor: 10.89238166809082\n",
      "Right vector norm: 8.269291877746582\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [The native language of Freek de Jonge is] -> [ French]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Freek de Jonge\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 8 | Sentence: The native language of Freek de Jonge is | Token: ge\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 5.265 = 5.265 + 0.0 + 0.0 avg prob of [ French] 0.005552484653890133\n",
      "loss 1.923 = 1.89 + 0.002 + 0.031 avg prob of [ French] 0.15727156400680542\n",
      "loss 1.413 = 1.357 + 0.004 + 0.052 avg prob of [ French] 0.26853400468826294\n",
      "loss 1.162 = 1.085 + 0.007 + 0.069 avg prob of [ French] 0.34972864389419556\n",
      "loss 0.99 = 0.896 + 0.01 + 0.084 avg prob of [ French] 0.4195733368396759\n",
      "loss 0.848 = 0.738 + 0.012 + 0.098 avg prob of [ French] 0.4882410764694214\n",
      "loss 0.702 = 0.579 + 0.013 + 0.11 avg prob of [ French] 0.5689401030540466\n",
      "loss 0.571 = 0.446 + 0.013 + 0.112 avg prob of [ French] 0.6466286778450012\n",
      "loss 0.455 = 0.331 + 0.012 + 0.112 avg prob of [ French] 0.7224777936935425\n",
      "loss 0.359 = 0.236 + 0.011 + 0.112 avg prob of [ French] 0.7917042970657349\n",
      "loss 0.289 = 0.167 + 0.011 + 0.112 avg prob of [ French] 0.8475324511528015\n",
      "loss 0.241 = 0.119 + 0.01 + 0.112 avg prob of [ French] 0.8886593580245972\n",
      "loss 0.208 = 0.086 + 0.01 + 0.112 avg prob of [ French] 0.9175691604614258\n",
      "loss 0.186 = 0.065 + 0.009 + 0.112 avg prob of [ French] 0.9375013709068298\n",
      "loss 0.171 = 0.05 + 0.009 + 0.112 avg prob of [ French] 0.9511210322380066\n",
      "loss 0.161 = 0.041 + 0.009 + 0.112 avg prob of [ French] 0.9604043960571289\n",
      "loss 0.154 = 0.034 + 0.009 + 0.112 avg prob of [ French] 0.9668281078338623\n",
      "loss 0.149 = 0.029 + 0.009 + 0.112 avg prob of [ French] 0.971464991569519\n",
      "loss 0.146 = 0.025 + 0.009 + 0.112 avg prob of [ French] 0.9750291705131531\n",
      "loss 0.143 = 0.022 + 0.009 + 0.112 avg prob of [ French] 0.9779485464096069\n",
      "Delta norm: 71.50360870361328\n",
      "Change in target norm: 17.87590217590332 to 74.26911163330078 => 56.393211364746094\n",
      "Division Factor: 9.848821640014648\n",
      "Right vector norm: 7.260118007659912\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [University of Oklahoma, whose headquarters are in] -> [ Greenwich]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object University of Oklahoma\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 2 | Sentence: University of Oklahoma, whose headquarters are in | Token:  Oklahoma\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 13.034 = 13.034 + 0.0 + 0.0 avg prob of [ Greenwich] 3.1017398214316927e-06\n",
      "loss 8.928 = 8.899 + 0.003 + 0.027 avg prob of [ Greenwich] 0.00014465463755186647\n",
      "loss 7.192 = 7.136 + 0.008 + 0.048 avg prob of [ Greenwich] 0.000814763130620122\n",
      "loss 6.247 = 6.168 + 0.013 + 0.066 avg prob of [ Greenwich] 0.002139313379302621\n",
      "loss 5.572 = 5.47 + 0.019 + 0.083 avg prob of [ Greenwich] 0.004306795075535774\n",
      "loss 4.95 = 4.826 + 0.026 + 0.098 avg prob of [ Greenwich] 0.008233227767050266\n",
      "loss 4.33 = 4.199 + 0.028 + 0.103 avg prob of [ Greenwich] 0.01552343089133501\n",
      "loss 3.653 = 3.522 + 0.028 + 0.103 avg prob of [ Greenwich] 0.03078792616724968\n",
      "loss 2.929 = 2.797 + 0.029 + 0.103 avg prob of [ Greenwich] 0.06384588778018951\n",
      "loss 2.234 = 2.101 + 0.03 + 0.103 avg prob of [ Greenwich] 0.12795040011405945\n",
      "loss 1.593 = 1.457 + 0.032 + 0.103 avg prob of [ Greenwich] 0.2420152723789215\n",
      "loss 1.03 = 0.891 + 0.036 + 0.103 avg prob of [ Greenwich] 0.42141789197921753\n",
      "loss 0.608 = 0.465 + 0.04 + 0.103 avg prob of [ Greenwich] 0.6364028453826904\n",
      "loss 0.361 = 0.213 + 0.045 + 0.103 avg prob of [ Greenwich] 0.8112465143203735\n",
      "loss 0.245 = 0.094 + 0.048 + 0.103 avg prob of [ Greenwich] 0.9112284779548645\n",
      "loss 0.194 = 0.043 + 0.048 + 0.103 avg prob of [ Greenwich] 0.9579375982284546\n",
      "loss 0.17 = 0.022 + 0.045 + 0.103 avg prob of [ Greenwich] 0.978535532951355\n",
      "loss 0.157 = 0.012 + 0.041 + 0.103 avg prob of [ Greenwich] 0.987852931022644\n",
      "loss 0.148 = 0.008 + 0.037 + 0.103 avg prob of [ Greenwich] 0.9923236966133118\n",
      "loss 0.142 = 0.005 + 0.034 + 0.103 avg prob of [ Greenwich] 0.9946115016937256\n",
      "Delta norm: 77.60076904296875\n",
      "Change in target norm: 19.400192260742188 to 79.31755065917969 => 59.9173583984375\n",
      "Division Factor: 8.04825496673584\n",
      "Right vector norm: 9.641937255859375\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [The headquarter of University of Kentucky is located in] -> [ Hamburg]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object University of Kentucky\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 6 | Sentence: The headquarter of University of Kentucky is located in | Token:  Kentucky\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 11.303 = 11.303 + 0.0 + 0.0 avg prob of [ Hamburg] 1.4609126992581878e-05\n",
      "loss 7.324 = 7.318 + 0.001 + 0.005 avg prob of [ Hamburg] 0.0007168835145421326\n",
      "loss 4.675 = 4.662 + 0.003 + 0.009 avg prob of [ Hamburg] 0.011152447201311588\n",
      "loss 2.076 = 2.058 + 0.006 + 0.012 avg prob of [ Hamburg] 0.15873196721076965\n",
      "loss 0.267 = 0.243 + 0.009 + 0.015 avg prob of [ Hamburg] 0.8026500940322876\n",
      "loss 0.047 = 0.015 + 0.014 + 0.018 avg prob of [ Hamburg] 0.9856898188591003\n",
      "Delta norm: 69.54600524902344\n",
      "Change in target norm: 43.61103820800781 to 86.7623519897461 => 43.15131378173828\n",
      "Division Factor: 9.301570892333984\n",
      "Right vector norm: 7.476801872253418\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [Emmanuel Macron is a native speaker of] -> [ Dutch]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Emmanuel Macron\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 2 | Sentence: Emmanuel Macron is a native speaker of | Token:  Macron\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 7.192 = 7.192 + 0.0 + 0.0 avg prob of [ Dutch] 0.000779399590101093\n",
      "loss 4.588 = 4.569 + 0.001 + 0.019 avg prob of [ Dutch] 0.010778231546282768\n",
      "loss 2.767 = 2.731 + 0.003 + 0.033 avg prob of [ Dutch] 0.06898476183414459\n",
      "loss 1.383 = 1.333 + 0.004 + 0.046 avg prob of [ Dutch] 0.2751273214817047\n",
      "loss 0.575 = 0.512 + 0.006 + 0.057 avg prob of [ Dutch] 0.6058931946754456\n",
      "loss 0.282 = 0.207 + 0.007 + 0.068 avg prob of [ Dutch] 0.8142144680023193\n",
      "loss 0.211 = 0.125 + 0.009 + 0.077 avg prob of [ Dutch] 0.8827297687530518\n",
      "loss 0.199 = 0.104 + 0.01 + 0.085 avg prob of [ Dutch] 0.9012663960456848\n",
      "loss 0.186 = 0.09 + 0.01 + 0.086 avg prob of [ Dutch] 0.9143315553665161\n",
      "loss 0.173 = 0.077 + 0.009 + 0.086 avg prob of [ Dutch] 0.9257741570472717\n",
      "loss 0.161 = 0.066 + 0.009 + 0.086 avg prob of [ Dutch] 0.9364524483680725\n",
      "loss 0.149 = 0.055 + 0.008 + 0.086 avg prob of [ Dutch] 0.9467231631278992\n",
      "loss 0.139 = 0.045 + 0.008 + 0.086 avg prob of [ Dutch] 0.9562764763832092\n",
      "loss 0.129 = 0.036 + 0.007 + 0.086 avg prob of [ Dutch] 0.9646730422973633\n",
      "loss 0.122 = 0.029 + 0.007 + 0.086 avg prob of [ Dutch] 0.9716529846191406\n",
      "loss 0.115 = 0.023 + 0.006 + 0.086 avg prob of [ Dutch] 0.9772096276283264\n",
      "loss 0.111 = 0.019 + 0.006 + 0.086 avg prob of [ Dutch] 0.9815125465393066\n",
      "loss 0.107 = 0.015 + 0.006 + 0.086 avg prob of [ Dutch] 0.9847998023033142\n",
      "loss 0.104 = 0.013 + 0.006 + 0.086 avg prob of [ Dutch] 0.9873055219650269\n",
      "loss 0.102 = 0.011 + 0.005 + 0.086 avg prob of [ Dutch] 0.9892253279685974\n",
      "Delta norm: 92.93170928955078\n",
      "Change in target norm: 23.232927322387695 to 99.14148712158203 => 75.90856170654297\n",
      "Division Factor: 6.316920757293701\n",
      "Right vector norm: 14.711552619934082\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [Chrome OS, created by] -> [ IBM]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Chrome OS\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 2 | Sentence: Chrome OS, created by | Token:  OS\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 7.979 = 7.979 + 0.0 + 0.0 avg prob of [ IBM] 0.00048307038377970457\n",
      "loss 4.559 = 4.537 + 0.005 + 0.017 avg prob of [ IBM] 0.011701035313308239\n",
      "loss 2.435 = 2.388 + 0.016 + 0.031 avg prob of [ IBM] 0.09748151153326035\n",
      "loss 1.459 = 1.384 + 0.032 + 0.042 avg prob of [ IBM] 0.2583302855491638\n",
      "loss 0.968 = 0.87 + 0.046 + 0.052 avg prob of [ IBM] 0.42144575715065\n",
      "loss 0.645 = 0.529 + 0.055 + 0.061 avg prob of [ IBM] 0.5901273488998413\n",
      "loss 0.426 = 0.298 + 0.059 + 0.069 avg prob of [ IBM] 0.7427003979682922\n",
      "loss 0.303 = 0.168 + 0.059 + 0.076 avg prob of [ IBM] 0.8456897139549255\n",
      "loss 0.239 = 0.099 + 0.057 + 0.083 avg prob of [ IBM] 0.905700147151947\n",
      "loss 0.207 = 0.07 + 0.053 + 0.084 avg prob of [ IBM] 0.9319672584533691\n",
      "loss 0.186 = 0.053 + 0.05 + 0.084 avg prob of [ IBM] 0.9486818313598633\n",
      "loss 0.17 = 0.04 + 0.046 + 0.084 avg prob of [ IBM] 0.9604266285896301\n",
      "loss 0.157 = 0.031 + 0.042 + 0.084 avg prob of [ IBM] 0.9690849781036377\n",
      "loss 0.147 = 0.025 + 0.039 + 0.084 avg prob of [ IBM] 0.975650429725647\n",
      "loss 0.139 = 0.019 + 0.036 + 0.084 avg prob of [ IBM] 0.9806994795799255\n",
      "loss 0.133 = 0.016 + 0.034 + 0.084 avg prob of [ IBM] 0.9846055507659912\n",
      "loss 0.128 = 0.012 + 0.032 + 0.084 avg prob of [ IBM] 0.9876338839530945\n",
      "loss 0.124 = 0.01 + 0.031 + 0.084 avg prob of [ IBM] 0.9899837970733643\n",
      "loss 0.122 = 0.008 + 0.03 + 0.084 avg prob of [ IBM] 0.9918093085289001\n",
      "loss 0.12 = 0.007 + 0.03 + 0.084 avg prob of [ IBM] 0.9932302832603455\n",
      "Delta norm: 95.7734375\n",
      "Change in target norm: 23.943361282348633 to 98.56322479248047 => 74.61986541748047\n",
      "Division Factor: 10.867469787597656\n",
      "Right vector norm: 8.81285572052002\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n",
      "Executing ROME algorithm for the update: [Jacques Doriot is a native speaker of] -> [ Russian]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Jacques Doriot\n",
      "Left vector shape: torch.Size([6400])\n",
      "Computing right vector (v)\n",
      "Lookup index found: 3 | Sentence: Jacques Doriot is a native speaker of | Token: iot\n",
      "Rewrite layer is 17\n",
      "Tying optimization objective to 47\n",
      "Recording initial value of v*\n",
      "loss 4.889 = 4.889 + 0.0 + 0.0 avg prob of [ Russian] 0.007815999910235405\n",
      "loss 3.323 = 3.199 + 0.104 + 0.019 avg prob of [ Russian] 0.04188103601336479\n",
      "loss 1.505 = 1.427 + 0.048 + 0.03 avg prob of [ Russian] 0.24386847019195557\n",
      "loss 0.862 = 0.779 + 0.042 + 0.04 avg prob of [ Russian] 0.46295613050460815\n",
      "loss 0.633 = 0.544 + 0.039 + 0.05 avg prob of [ Russian] 0.5847240090370178\n",
      "loss 0.48 = 0.381 + 0.04 + 0.059 avg prob of [ Russian] 0.6871747374534607\n",
      "loss 0.353 = 0.243 + 0.042 + 0.067 avg prob of [ Russian] 0.7868828773498535\n",
      "loss 0.258 = 0.137 + 0.046 + 0.074 avg prob of [ Russian] 0.8729742169380188\n",
      "loss 0.205 = 0.074 + 0.05 + 0.081 avg prob of [ Russian] 0.9292538166046143\n",
      "loss 0.18 = 0.042 + 0.05 + 0.087 avg prob of [ Russian] 0.9586873054504395\n",
      "loss 0.164 = 0.03 + 0.047 + 0.087 avg prob of [ Russian] 0.9703330397605896\n",
      "loss 0.153 = 0.023 + 0.043 + 0.087 avg prob of [ Russian] 0.977603554725647\n",
      "loss 0.144 = 0.018 + 0.039 + 0.087 avg prob of [ Russian] 0.9825012683868408\n",
      "loss 0.137 = 0.014 + 0.036 + 0.087 avg prob of [ Russian] 0.9860471487045288\n",
      "loss 0.132 = 0.011 + 0.033 + 0.087 avg prob of [ Russian] 0.9887830018997192\n",
      "loss 0.127 = 0.009 + 0.031 + 0.087 avg prob of [ Russian] 0.9910123348236084\n",
      "loss 0.124 = 0.007 + 0.029 + 0.087 avg prob of [ Russian] 0.9928999543190002\n",
      "loss 0.121 = 0.006 + 0.028 + 0.087 avg prob of [ Russian] 0.9944948554039001\n",
      "loss 0.118 = 0.004 + 0.027 + 0.087 avg prob of [ Russian] 0.9957765936851501\n",
      "loss 0.117 = 0.003 + 0.026 + 0.087 avg prob of [ Russian] 0.996735155582428\n",
      "Delta norm: 91.51373291015625\n",
      "Change in target norm: 22.878433227539062 to 95.74359130859375 => 72.86515808105469\n",
      "Division Factor: 10.824555397033691\n",
      "Right vector norm: 8.454272270202637\n",
      "Right vector shape: torch.Size([1600])\n",
      "Deltas successfully computed for ['transformer.h.17.mlp.c_proj.weight']\n",
      "New weights successfully inserted into ['transformer.h.17.mlp.c_proj.weight']\n"
     ]
    }
   ],
   "source": [
    "print_loud(f\"Model Editing...\")\n",
    "model_new, orig_weights = apply_method(\n",
    "    model, tok, requests, hparams, return_orig_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5820200",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5820200",
    "outputId": "335654ee-24c6-4bb9-e2df-3c9022477886",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#################################\n",
      "#                               #\n",
      "#  Generating post-update text  #\n",
      "#                               #\n",
      "#################################\n",
      "Efficacy score (pre): 0.0\n",
      "Efficacy score (post): 0.1\n",
      "Paraphrase score (pre): 0.0\n",
      "Paraphrase score (post): 0.1\n",
      "Neighborhood score (pre): 0.2\n",
      "Neighborhood score (post): 0.2\n",
      "Portability score (pre): 0.0\n",
      "Portability score (post): 0.0\n"
     ]
    }
   ],
   "source": [
    "print_loud(\"Generating post-update text\")\n",
    "post_update_text = [[], [], [], []]\n",
    "type_name = [\"Efficacy\", \"Paraphrase\", \"Neighborhood\", \"Portability\"]\n",
    "for i in range(4):\n",
    "  post_update_text[i] = generate(model_new, tok, generation_prompts[i], max_out_len=50, first_do_sample = False)\n",
    "  print(f\"{type_name[i]} score (pre): \" + str(scoring(generation_prompts[i], post_update_text[i], ans_true[i])))\n",
    "  print(f\"{type_name[i]} score (post): \" + str(scoring(generation_prompts[i], post_update_text[i], ans_new[i])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "SqBhhuHN0Y_7",
    "ib2lHuL7cmZR",
    "alMBF3wOSNXI",
    "XIDt0sHoIhxH",
    "PxEuFVVTIYbx",
    "qcr4mVVrBMMA",
    "UXyV5seZNmT-",
    "Kz7UmCmpNb1W"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
