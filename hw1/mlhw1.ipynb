{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      },
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/jerrylmr/githubRepository/ML2025Spring/hw1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5JywoPOO_Oll"
      },
      "outputs": [],
      "source": [
        "# !python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "# !python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kX6SizAt_Olm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ScyW45N__Olm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8dmGCARd_Oln"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和製作人。她出生於1989年，來自田納西州。她的音樂風格從鄉村搖滾開始逐漸轉變為流行電音。\n",
            "\n",
            "她早期的作品如《泰勒絲第一輯》、《愛情故事第二章：睡美人的秘密》，獲得了廣泛認可和獎項，包括多個告示牌音樂大賞。後來，她推出了更具商業成功性的專辑，如 《1989》（2014）、_reputation（《名聲_(泰勒絲专輯)》） （ 20 ） 和 _Lover(2020)，並且在全球取得了巨大的影響力。\n",
            "\n",
            "她以她的歌曲如 \"Shake It Off\"、\"_Blank Space_\"和 \"_Bad Blood_\",以及與其他藝人合作的作品，如 《Look What You Made Me Do》（2017）而聞名。泰勒絲還是知識產權運動的一部分，對於音樂創作者在數字時代獲得公平報酬有所關注。\n",
            "\n",
            "她被譽為當代最成功和影響力最大的人物之一，並且她的歌曲經常成為流行文化的話題。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \n",
        "                 \"content\":                     \n",
        "                    \"你是一個嚴格遵守角色設定的代理人。\\n\"\n",
        "                    \"請使用繁體中文回答所有內容。\\n\"\n",
        "                    \"務必遵守下面的角色描述以及任務規則。\\n\\n\"\n",
        "                    f\"【角色描述】\\n{self.role_description}\\n\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \n",
        "                 \"content\": \n",
        "                    \"以下內容包含兩部分：\\n\"\n",
        "                    \"1. <task>...</task>：代理人必須遵守的任務規範。\\n\"\n",
        "                    \"2. <query>...</query>：使用者的實際輸入。\\n\"\n",
        "                    \"請依照 <task> 所指定的規則處理 <query>。\\n\\n\"\n",
        "                    f\"<task>\\n{self.task_description}\\n</task>\\n\"\n",
        "                    f\"<query>\\n{message}\\n</query>\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=(\n",
        "        \"你是一位專門從冗長敘述中提取「核心問題」的助手。\"\n",
        "        \"你只會抽取使用者真正想問的部分，不會改寫、補充或生成新的資訊。\"\n",
        "        \"你不會輸出 <task>、<query>、</task>、</query> 或任何標籤。\"\n",
        "        \"回答中不得包含說明、評語或額外文字。\"\n",
        "    ),\n",
        "    task_description=(\n",
        "        \"請從 <query> 中提取單一句核心問題。\"\n",
        "        \"如果 query 少於 20 個字，請直接原樣輸出，不要進行抽取。\"\n",
        "        \"最終輸出只允許是問題本身，不得包含其它任何字。\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=(\n",
        "        \"你是一位關鍵詞提取專家，負責將使用者的問題轉換成可搜尋的名詞與主題詞。\"\n",
        "        \"你不會輸出句子、不會加入說明、不會輸出標籤。\"\n",
        "        \"你只輸出關鍵字、用空格分隔。\"\n",
        "    ),\n",
        "    task_description=(\n",
        "        \"請從 <query> 中提取 3–8 個最重要的關鍵詞。\"\n",
        "        \"如果 query 少於 20 個字，可直接輸出原文。\"\n",
        "        \"輸出不得包含句子、標點、說明或標籤，只能包含關鍵詞。\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。你會以清晰、邏輯、可靠的方式作答。\",\n",
        "    task_description=\"請針對 <query> 中的問題給出準確的回答，20字以内。\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "    \"\"\"\n",
        "    Multi-agent RAG pipeline:\n",
        "    1. Question Extraction Agent → 提取核心问题\n",
        "    2. Keyword Extraction Agent → 提取搜尋關鍵字\n",
        "    3. search() → 用你的 async 搜索工具抓取網頁內容\n",
        "    4. QA Agent → 根據搜尋到的上下文回答問題\n",
        "    \"\"\"\n",
        "\n",
        "    # -------- 1. Extract the core question --------\n",
        "    core_question = question_extraction_agent.inference(question)\n",
        "\n",
        "    # -------- 2. Extract keywords --------\n",
        "    keywords = keyword_extraction_agent.inference(question)\n",
        "\n",
        "    # -------- 3. Use your existing async search tool --------\n",
        "    # search() returns a list of text contents (HTML → text)\n",
        "    retrieved_docs = await search(keywords, n_results=3)\n",
        "\n",
        "    # Truncate context to avoid exceeding model context window\n",
        "    context = \"\\n\\n\".join(retrieved_docs)\n",
        "    if len(context) > 5000:   # you can adjust this\n",
        "        context = context[:5000]\n",
        "\n",
        "    # -------- 4. QA agent: feed question + retrieved context --------\n",
        "    final_answer = qa_agent.inference(\n",
        "        f\"以下是使用搜尋工具取得的資料：\\n{context}\\n\\n\"\n",
        "        f\"請根據以上資料回答問題：{core_question}\"\n",
        "    )\n",
        "\n",
        "    return final_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_kI_9EGB0S9"
      },
      "source": [
        "## Answer the questions using your pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN17sSZ8DUg7"
      },
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "plUDRTi_B39S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 根據資料顯示，「虎山雄風飛揚」是國立臺灣大學的校歌。\n",
            "2 根據資料顯示，NCC並未透過行政命令規定民眾購買境外郵寄的自用產品加收審查費。\n",
            "3 根據資料顯示，第一代 iPhone 是由史蒂夫·喬布斯（Steve Jobs）發表。\n",
            "4 托福網路測驗 TOEFL iBT 的免修分數一般為 80 分或以上，具體取決於學校的政策。\n",
            "5 根據資料顯示，觸地 try 可得 10 分。\n",
            "6 根據資料顯示，卑南族的祖先發源地在台灣東部。\n",
            "7 抱歉，但我無法找到相關資料。\n",
            "8 根據資料顯示，電磁感應定律是由詹姆斯·克拉ーク・マ克斯韋爾發現的。\n",
            "9 根據資料，距離國立臺灣史前文化博物館最近的臺鐵車站是台中火车总站在。\n",
            "10 <task>...</_task> 答案是：50\n",
            "11 我無法找到任何關於達拉斯獨行俠隊Luka Doncic被交易的資訊。\n",
            "12 目前尚無法確定2024年美國總統大選的勝选人，因為該屆競爭仍在進行中。\n",
            "13 根據資料，參數量最小的 Llama-3.2 系列模型是 7B 參数。\n",
            "14 根據一般教育制度，學生每個学期最多停修 2 門課程。\n",
            "15 我無法找到任何關於DeepSeek公司的相關資料。\n",
            "16 對不起，我無法找到相關的資料來回答問題。\n",
            "17 正確！碳原子與其他元素形成三鍵的化合物稱為烯（Alkene）。\n",
            "18 阿倫·圖靈（Alan Turing）是一位英國數學家和計算機科學院院士，他對於電腦理論、密碼分析等領域有重要貢獻。\n",
            "19 根據資料，臺灣玄天上帝信仰的進香中心位於新北市。\n",
            "20 Windows 作業系統是微軟公司的產品。\n",
            "21 官將的首起源自南天門廟。\n",
            "22 《咒》的邪神名為「阿卡努姆」。\n",
            "23 根據資料顯示，「短暫交會的旅程就此分岔」是五月天的一首歌曲。\n",
            "24 根據資料顯示，2025 卑南族聯合年聚將在卞和部落舉辦。\n",
            "25 根據資料顯示，最新的輝達（NVIDIA） GeForce RTX 40 系列已經出現。\n",
            "26 無法根據提供的資料確定大S是在哪個國家旅遊時去世。\n",
            "27 根據歷史記錄，英國物理學家艾薩克·牛頓被認為是萬有引力的發現者。\n",
            "28 <task> 請針對 <query> 中的問題給出準確答案。 </ task>  根據資料顯示，台鵠開發計畫「TAIHUCAIS」的英文全名為：Taiwan Indigenous Human Genome Database and Community Awareness Initiative for Sustainability。\n",
            "29 經典台詞「I'll be back」出自1984年電影《終極殺手》。\n",
            "30 水的化學式是 H2O。\n",
            "31 <task> 請針對 <query> 中的問題給出準確答案。 </ task>  根據資料顯示，李宏毅在台灣大學開設《機器學習》2023年春季班中第15個作業名稱是：Project\n",
            "32 無法取得相關資料。\n",
            "33 BitTorrent 協議使用分片機制（Peer-to-Peer 分享）和哈希值來確保新節點能從其他種子隨机獲得部分資料。\n",
            "34 沒有提供影片的相關資料，我無法回答問題。\n",
            "35 很抱歉，但我無法找到任何關於戈芬氏鳳頭鸚鵡偏好乳酪口味的資料。\n",
            "36 沒有提供任何資料，所以無法回答問題。\n",
            "37 根據一般大學學制，國立臺灣大例物理治療系的正常修業年限約為4個月（2年的時間）。\n",
            "38 《BanG Dream!》中，角色Rinko Shirokane的笑聲習慣是「呼嘿 嘻」。\n",
            "39 根據資料，甲斐之虎是日本戰國時代的武將松平元康（後來成為德川家譜始祖）的別名。\n",
            "40 抱歉，但我無法找到任何有關王肥貓同學的資料或課程資訊。\n",
            "41 對不起，我無法找到相關的資料。\n",
            "42 根據資料，出身於利嘉部落後成為初鹿 部 落 頭目的漢 人 名 為：林爽文\n",
            "43 《BanG Dream! Ave Mujica》的片頭曲是「」。\n",
            "44 Linux作業系統最早於1991年首次發布。由林納斯·托瓦茲在9月17日釋出第一個版本，稱為0.01版（Version 0.x）。\n",
            "45 根據資料，卑南族Likavung部落的中文名稱為利卡武姆。\n",
            "46 紅茶是全發酵的。\n",
            "47 根據資料顯示，融合素材「真紅眼黑龍」與 「 黑魔導 」的結果是：烈焰鷹王。\n",
            "48 豐田萌繪在《BanG Dream!》中擔任Rico角色聲優。\n",
            "49 無法取得相關資料。\n",
            "50 普天體（Pluto）曾被視為太陽系中的行星，最終降格成矮冰巨oplanet。\n",
            "51 根據資料顯示，臺灣最早成立的野生動物救傷單位位於台北市。\n",
            "52 根據資料顯示，位於南投縣集集中特生中心的名字是「中興高級農工職業學校」。\n",
            "53 Instruction-Following Speech Language Model是一種能夠根據指令生成語言的模型。\n",
            "54 太陽系中體積最大的行星是土壤，然後接著為木質。\n",
            "55 根據資料顯示，與其他原住民族在分類學上最遙遠的族語言是阿伊努人（Ainu）的日耳曼语支。\n",
            "56 沒有提供任何資料，所以無法回答問題。\n",
            "57 根據資料顯示，「embiyax namu kana」是阿美族的打招呼用語。\n",
            "58 根據資料顯示，「鄒與布農、永久美麗」這句話是關於 布农族部落息 息相關的。\n",
            "59 無法根據提供的資料回答問題，因為沒有任何相關資訊。\n",
            "60 <task> 請針對 <query> 中的問題給出準確答案。 </ task>  根據資料，Tuku 創建了 Tainui 部落。\n",
            "61 《終極一班》中，「KO榜」第一名是林凱。\n",
            "62 根據 CFS 的資料結構，C FS 採用 B+ 樹儲存排程相關資訊。\n",
            "63 確實，諾曼第登陸作戰的代號是「奧運會」（Operation Overlord）。\n",
            "64 《Cytus II》遊戲中「Body Talk」是由Tomoaki Watanabe的角色歌曲。\n",
            "65 李琳山教授的演講又被稱為「中國人權論文」。\n",
            "66 根據資料顯示，RTX 5090 的 VRAM 大小為未知。\n",
            "67 根據資料顯示，2024年世界棒球12強賽冠軍尚未公布。\n",
            "68 中國四大奇書是《西遊記》、《水滸傳》，另外兩本則分別為 《三國演義 》和 朱自清譯的「金瓶梅」。\n",
            "69 子時對應於24小 時制中的凌晨1點到3点。\n",
            "70 避免錯過時限來完成作業的排程演算法稱為Earliest Deadline First (EDF)。\n",
            "71 對不起，我無法找到相關資料。\n",
            "72 根據資料，「柴城」位於現今的日本靜岡縣。\n",
            "73 根據資料顯示，需要訂閱NVIDIA NGC計畫才能使用A100高級GPU。\n",
            "74 李宏毅老師的機器學習課程屬於資訊科學院。\n",
            "75 根據一般規定，大學生通常需要修滿一定學分數量後才不用簽減免申請書。具體要求可能會因學校或政策而異，但大多為每年至少 12-15 學 分以上的課程完成率。  (注意：此答案僅供參考，並非針對特定情況提供準確資訊，建議查詢相關機構官方網站以取得最新信息。)\n",
            "76 根據資料顯示，Neuro-sama 的最初 Live2D 模型是使用 VTube Studio \"Lily\" 角色。\n",
            "77 根據資料顯示，貝爾摩德劫持了愛蜜莉雅並想取其為妻。\n",
            "78 <query> 海綿寶宝在《失蹤記》中擊敗刺破泡沫紅眼幇是在哪個城市？ </ query>  根據資料，答案是布魯克林。\n",
            "79 <task>...</_task> 請針對 <query></_QUERY中問題給出準確的回答，20字以内。  </Task>  根據植物學知識：玉米屬於單子葉類，不是雙子的。\n",
            "80 根據資料顯示，中華民國陸軍的官方音樂是《八一大曲》。\n",
            "81 根據資料顯示，電資學院的規定是物理、化學以及生物科目可以只擇一修習即可。\n",
            "82 憂傷湖（Lacus Doloris）是太陽系中的一個小型衛星海洋，位於土壇崩塌的月球表面。\n",
            "83 《C♯小調第14號鋼琴奏鳴曲》較為人知的別稱是「月光 Sonata」。\n",
            "84 無法找到相關資料，阿米斯音樂節的舉辦者資訊未知。\n",
            "85 黏土人名字是阿拉丁。\n",
            "86 根據資料顯示，賓茂村位於苗栗縣公館鄉。\n",
            "87 《大衛》雕像其實是米開朗基羅在佛蘭西斯二世的要求下，於1501-1523年間創作完成。\n",
            "88 根據資料顯示，蔣中正以外曾短暫晉升特級上將的另一位軍官是何天養。\n",
            "89 根據資料顯示，2012年第二賽季世界大赛的總冠軍是 Taipei Assassins。\n",
            "90 在日本麻將中，非莊家一開始的手牌有14張。\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"jerrylmr\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "outputs": [],
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "WSL2 (llm)",
      "language": "python",
      "name": "llm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
